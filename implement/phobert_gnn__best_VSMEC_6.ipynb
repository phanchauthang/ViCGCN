{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qm_DII13AYh"
      },
      "source": [
        "# install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIQOehLKtNXL",
        "outputId": "0be0d894-73e8-435e-d924-6417a62d28f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dgl-cu102\n",
            "  Downloading dgl_cu102-0.6.1-cp38-cp38-manylinux1_x86_64.whl (36.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 36.8 MB 136 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from dgl-cu102) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from dgl-cu102) (1.7.3)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.8/dist-packages (from dgl-cu102) (2.8.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from dgl-cu102) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->dgl-cu102) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->dgl-cu102) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->dgl-cu102) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->dgl-cu102) (1.24.3)\n",
            "Installing collected packages: dgl-cu102\n",
            "Successfully installed dgl-cu102-0.6.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.8.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 15.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 56.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --pre dgl-cu102\n",
        "!pip install --upgrade gdown\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPXNkDaQtQ6r",
        "outputId": "6fdefe6e-ca54-412b-ee2b-bf33f1412467"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NJ-Icr_F4IQe5LPSwg-_IRvgMo8LrZ3u\n",
            "To: /content/data.zip\n",
            "\r  0% 0.00/1.24M [00:00<?, ?B/s]\r100% 1.24M/1.24M [00:00<00:00, 141MB/s]\n",
            "Archive:  /content/data.zip\n",
            "   creating: data/\n",
            "   creating: data/corpus/\n",
            "  inflating: data/corpus/ViCTSD.txt  \n",
            "  inflating: data/corpus/VSFC.txt    \n",
            "  inflating: data/corpus/VSMEC.txt   \n",
            "  inflating: data/corpus/VSMEC_6.txt  \n",
            "  inflating: data/ViCTSD.txt         \n",
            "  inflating: data/VSFC.txt           \n",
            "  inflating: data/VSMEC.txt          \n",
            "  inflating: data/VSMEC_6.txt        \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ijisDiO0ji7wmDPPiCIEZlFpiAXxK_4p\n",
            "To: /content/re.txt\n",
            "100% 133/133 [00:00<00:00, 258kB/s]\n"
          ]
        }
      ],
      "source": [
        "# %cd 'content'\n",
        "# https://drive.google.com/file/d/1NJ-Icr_F4IQe5LPSwg-_IRvgMo8LrZ3u/view?usp=share_link\n",
        "!gdown \"1NJ-Icr_F4IQe5LPSwg-_IRvgMo8LrZ3u\"\n",
        "!unzip '/content/data.zip'\n",
        "# https://drive.google.com/file/d/1ijisDiO0ji7wmDPPiCIEZlFpiAXxK_4p/view?usp=share_link\n",
        "!gdown '1ijisDiO0ji7wmDPPiCIEZlFpiAXxK_4p'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4MzqM1QSEFu",
        "outputId": "3b3e8eda-981a-4a89-fd30-acba9c79256c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,235 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,143 kB]\n",
            "Fetched 3,659 kB in 5s (780 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "20 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following packages have been kept back:\n",
            "  libcudnn8 libcudnn8-dev libnccl-dev libnccl2\n",
            "The following packages will be upgraded:\n",
            "  binutils binutils-common binutils-x86-64-linux-gnu libasn1-8-heimdal\n",
            "  libbinutils libgssapi3-heimdal libhcrypto4-heimdal libheimbase1-heimdal\n",
            "  libheimntlm0-heimdal libhx509-5-heimdal libkrb5-26-heimdal\n",
            "  libroken18-heimdal libwind0-heimdal linux-libc-dev login passwd\n",
            "16 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 5,454 kB of archives.\n",
            "After this operation, 12.3 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 login amd64 1:4.5-1ubuntu2.5 [307 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 passwd amd64 1:4.5-1ubuntu2.5 [818 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.30-21ubuntu1~18.04.8 [1,839 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-common amd64 2.30-21ubuntu1~18.04.8 [197 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils amd64 2.30-21ubuntu1~18.04.8 [3,388 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libbinutils amd64 2.30-21ubuntu1~18.04.8 [488 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libroken18-heimdal amd64 7.5.0+dfsg-1ubuntu0.2 [41.8 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libasn1-8-heimdal amd64 7.5.0+dfsg-1ubuntu0.2 [174 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libheimbase1-heimdal amd64 7.5.0+dfsg-1ubuntu0.2 [29.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libhcrypto4-heimdal amd64 7.5.0+dfsg-1ubuntu0.2 [85.9 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libwind0-heimdal amd64 7.5.0+dfsg-1ubuntu0.2 [48.0 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libhx509-5-heimdal amd64 7.5.0+dfsg-1ubuntu0.2 [108 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libkrb5-26-heimdal amd64 7.5.0+dfsg-1ubuntu0.2 [207 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libheimntlm0-heimdal amd64 7.5.0+dfsg-1ubuntu0.2 [14.7 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgssapi3-heimdal amd64 7.5.0+dfsg-1ubuntu0.2 [96.3 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 linux-libc-dev amd64 4.15.0-200.211 [997 kB]\n",
            "Fetched 5,454 kB in 3s (1,847 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 16.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../login_1%3a4.5-1ubuntu2.5_amd64.deb ...\n",
            "Unpacking login (1:4.5-1ubuntu2.5) over (1:4.5-1ubuntu2.3) ...\n",
            "Setting up login (1:4.5-1ubuntu2.5) ...\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../passwd_1%3a4.5-1ubuntu2.5_amd64.deb ...\n",
            "Unpacking passwd (1:4.5-1ubuntu2.5) over (1:4.5-1ubuntu2.3) ...\n",
            "Setting up passwd (1:4.5-1ubuntu2.5) ...\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../00-binutils-x86-64-linux-gnu_2.30-21ubuntu1~18.04.8_amd64.deb ...\n",
            "Unpacking binutils-x86-64-linux-gnu (2.30-21ubuntu1~18.04.8) over (2.30-21ubuntu1~18.04.7) ...\n",
            "Preparing to unpack .../01-binutils-common_2.30-21ubuntu1~18.04.8_amd64.deb ...\n",
            "Unpacking binutils-common:amd64 (2.30-21ubuntu1~18.04.8) over (2.30-21ubuntu1~18.04.7) ...\n",
            "Preparing to unpack .../02-binutils_2.30-21ubuntu1~18.04.8_amd64.deb ...\n",
            "Unpacking binutils (2.30-21ubuntu1~18.04.8) over (2.30-21ubuntu1~18.04.7) ...\n",
            "Preparing to unpack .../03-libbinutils_2.30-21ubuntu1~18.04.8_amd64.deb ...\n",
            "Unpacking libbinutils:amd64 (2.30-21ubuntu1~18.04.8) over (2.30-21ubuntu1~18.04.7) ...\n",
            "Preparing to unpack .../04-libroken18-heimdal_7.5.0+dfsg-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libroken18-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) over (7.5.0+dfsg-1ubuntu0.1) ...\n",
            "Preparing to unpack .../05-libasn1-8-heimdal_7.5.0+dfsg-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libasn1-8-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) over (7.5.0+dfsg-1ubuntu0.1) ...\n",
            "Preparing to unpack .../06-libheimbase1-heimdal_7.5.0+dfsg-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libheimbase1-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) over (7.5.0+dfsg-1ubuntu0.1) ...\n",
            "Preparing to unpack .../07-libhcrypto4-heimdal_7.5.0+dfsg-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libhcrypto4-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) over (7.5.0+dfsg-1ubuntu0.1) ...\n",
            "Preparing to unpack .../08-libwind0-heimdal_7.5.0+dfsg-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libwind0-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) over (7.5.0+dfsg-1ubuntu0.1) ...\n",
            "Preparing to unpack .../09-libhx509-5-heimdal_7.5.0+dfsg-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libhx509-5-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) over (7.5.0+dfsg-1ubuntu0.1) ...\n",
            "Preparing to unpack .../10-libkrb5-26-heimdal_7.5.0+dfsg-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libkrb5-26-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) over (7.5.0+dfsg-1ubuntu0.1) ...\n",
            "Preparing to unpack .../11-libheimntlm0-heimdal_7.5.0+dfsg-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libheimntlm0-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) over (7.5.0+dfsg-1ubuntu0.1) ...\n",
            "Preparing to unpack .../12-libgssapi3-heimdal_7.5.0+dfsg-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libgssapi3-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) over (7.5.0+dfsg-1ubuntu0.1) ...\n",
            "Preparing to unpack .../13-linux-libc-dev_4.15.0-200.211_amd64.deb ...\n",
            "Unpacking linux-libc-dev:amd64 (4.15.0-200.211) over (4.15.0-194.205) ...\n",
            "Setting up linux-libc-dev:amd64 (4.15.0-200.211) ...\n",
            "Setting up libroken18-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) ...\n",
            "Setting up binutils-common:amd64 (2.30-21ubuntu1~18.04.8) ...\n",
            "Setting up libheimbase1-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) ...\n",
            "Setting up libwind0-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) ...\n",
            "Setting up libbinutils:amd64 (2.30-21ubuntu1~18.04.8) ...\n",
            "Setting up libasn1-8-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) ...\n",
            "Setting up libhcrypto4-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) ...\n",
            "Setting up libhx509-5-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) ...\n",
            "Setting up libkrb5-26-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) ...\n",
            "Setting up libheimntlm0-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) ...\n",
            "Setting up binutils-x86-64-linux-gnu (2.30-21ubuntu1~18.04.8) ...\n",
            "Setting up binutils (2.30-21ubuntu1~18.04.8) ...\n",
            "Setting up libgssapi3-heimdal:amd64 (7.5.0+dfsg-1ubuntu0.2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "/content/install\n",
            "--2022-12-29 07:50:15--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.39.144\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.39.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 190 [application/octet-stream]\n",
            "Saving to: ‘cuda-ubuntu1804.pin’\n",
            "\n",
            "cuda-ubuntu1804.pin 100%[===================>]     190  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-29 07:50:15 (11.5 MB/s) - ‘cuda-ubuntu1804.pin’ saved [190/190]\n",
            "\n",
            "Executing: /tmp/apt-key-gpghome.LcqIhX3KWc/gpg.1.sh --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
            "gpg: requesting key from 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub'\n",
            "gpg: key F60F4B3D7FA2AF80: \"cudatools <cudatools@nvidia.com>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n",
            "Hit:1 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list:53 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list:53 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1\n",
            "Hit:1 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list:53 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list:53 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  cuda-command-line-tools-10-2 cuda-compiler-10-2 cuda-cudart-10-2\n",
            "  cuda-cudart-dev-10-2 cuda-cufft-10-2 cuda-cufft-dev-10-2 cuda-cuobjdump-10-2\n",
            "  cuda-cupti-10-2 cuda-cupti-dev-10-2 cuda-curand-10-2 cuda-curand-dev-10-2\n",
            "  cuda-cusolver-10-2 cuda-cusolver-dev-10-2 cuda-cusparse-10-2\n",
            "  cuda-cusparse-dev-10-2 cuda-demo-suite-10-2 cuda-documentation-10-2\n",
            "  cuda-driver-dev-10-2 cuda-gdb-10-2 cuda-libraries-10-2\n",
            "  cuda-libraries-dev-10-2 cuda-license-10-2 cuda-memcheck-10-2\n",
            "  cuda-misc-headers-10-2 cuda-npp-10-2 cuda-npp-dev-10-2 cuda-nsight-10-2\n",
            "  cuda-nsight-compute-10-2 cuda-nsight-systems-10-2 cuda-nvcc-10-2\n",
            "  cuda-nvdisasm-10-2 cuda-nvgraph-10-2 cuda-nvgraph-dev-10-2 cuda-nvjpeg-10-2\n",
            "  cuda-nvjpeg-dev-10-2 cuda-nvml-dev-10-2 cuda-nvprof-10-2 cuda-nvprune-10-2\n",
            "  cuda-nvrtc-10-2 cuda-nvrtc-dev-10-2 cuda-nvtx-10-2 cuda-nvvp-10-2\n",
            "  cuda-runtime-10-2 cuda-samples-10-2 cuda-sanitizer-api-10-2\n",
            "  cuda-toolkit-10-2 cuda-tools-10-2 cuda-visual-tools-10-2 freeglut3\n",
            "  freeglut3-dev libcublas-dev libcublas10 libxi-dev libxmu-dev libxmu-headers\n",
            "  x11proto-input-dev\n",
            "The following NEW packages will be installed:\n",
            "  cuda-10-2 cuda-command-line-tools-10-2 cuda-compiler-10-2 cuda-cudart-10-2\n",
            "  cuda-cudart-dev-10-2 cuda-cufft-10-2 cuda-cufft-dev-10-2 cuda-cuobjdump-10-2\n",
            "  cuda-cupti-10-2 cuda-cupti-dev-10-2 cuda-curand-10-2 cuda-curand-dev-10-2\n",
            "  cuda-cusolver-10-2 cuda-cusolver-dev-10-2 cuda-cusparse-10-2\n",
            "  cuda-cusparse-dev-10-2 cuda-demo-suite-10-2 cuda-documentation-10-2\n",
            "  cuda-driver-dev-10-2 cuda-gdb-10-2 cuda-libraries-10-2\n",
            "  cuda-libraries-dev-10-2 cuda-license-10-2 cuda-memcheck-10-2\n",
            "  cuda-misc-headers-10-2 cuda-npp-10-2 cuda-npp-dev-10-2 cuda-nsight-10-2\n",
            "  cuda-nsight-compute-10-2 cuda-nsight-systems-10-2 cuda-nvcc-10-2\n",
            "  cuda-nvdisasm-10-2 cuda-nvgraph-10-2 cuda-nvgraph-dev-10-2 cuda-nvjpeg-10-2\n",
            "  cuda-nvjpeg-dev-10-2 cuda-nvml-dev-10-2 cuda-nvprof-10-2 cuda-nvprune-10-2\n",
            "  cuda-nvrtc-10-2 cuda-nvrtc-dev-10-2 cuda-nvtx-10-2 cuda-nvvp-10-2\n",
            "  cuda-runtime-10-2 cuda-samples-10-2 cuda-sanitizer-api-10-2\n",
            "  cuda-toolkit-10-2 cuda-tools-10-2 cuda-visual-tools-10-2 freeglut3\n",
            "  freeglut3-dev libcublas-dev libcublas10 libxi-dev libxmu-dev libxmu-headers\n",
            "  x11proto-input-dev\n",
            "0 upgraded, 57 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 1,430 MB of archives.\n",
            "After this operation, 3,476 MB of additional disk space will be used.\n",
            "Get:1 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-license-10-2 10.2.89-1 [16.4 kB]\n",
            "Get:2 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-misc-headers-10-2 10.2.89-1 [1,111 kB]\n",
            "Get:3 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nvcc-10-2 10.2.89-1 [37.4 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\n",
            "Get:5 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-cuobjdump-10-2 10.2.89-1 [88.5 kB]\n",
            "Get:6 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nvprune-10-2 10.2.89-1 [39.5 kB]\n",
            "Get:7 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-compiler-10-2 10.2.89-1 [2,546 B]\n",
            "Get:8 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nvdisasm-10-2 10.2.89-1 [22.2 MB]\n",
            "Get:9 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-gdb-10-2 10.2.89-1 [2,769 kB]\n",
            "Get:10 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nvprof-10-2 10.2.89-1 [1,651 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 freeglut3-dev amd64 2.8.1-3 [124 kB]\n",
            "Get:12 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-sanitizer-api-10-2 10.2.89-1 [2,161 kB]\n",
            "Get:13 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-memcheck-10-2 10.2.89-1 [139 kB]\n",
            "Get:14 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-cudart-10-2 10.2.89-1 [111 kB]\n",
            "Get:15 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-driver-dev-10-2 10.2.89-1 [11.8 kB]\n",
            "Get:16 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-cudart-dev-10-2 10.2.89-1 [491 kB]\n",
            "Get:17 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-cupti-10-2 10.2.89-1 [8,169 kB]\n",
            "Get:18 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-cupti-dev-10-2 10.2.89-1 [2,197 kB]\n",
            "Get:19 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nvtx-10-2 10.2.89-1 [38.9 kB]\n",
            "Get:20 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-command-line-tools-10-2 10.2.89-1 [27.0 kB]\n",
            "Get:21 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nsight-10-2 10.2.89-1 [2,598 B]\n",
            "Get:22 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nvvp-10-2 10.2.89-1 [2,548 B]\n",
            "Get:23 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nvrtc-10-2 10.2.89-1 [6,413 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxmu-headers all 2:1.1.2-2 [54.3 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxmu-dev amd64 2:1.1.2-2 [49.0 kB]\n",
            "Get:26 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nvrtc-dev-10-2 10.2.89-1 [8,840 B]\n",
            "Get:27 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-cusolver-10-2 10.2.89-1 [85.6 MB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-input-dev all 2018.4-4 [2,620 B]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxi-dev amd64 2:1.7.9-1 [186 kB]\n",
            "Get:30 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-cusolver-dev-10-2 10.2.89-1 [15.2 MB]\n",
            "Get:31 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcublas10 10.2.3.254-1 [43.1 MB]\n",
            "Get:32 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcublas-dev 10.2.3.254-1 [42.4 MB]\n",
            "Get:33 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-cufft-10-2 10.2.89-1 [87.8 MB]\n",
            "Get:34 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-cufft-dev-10-2 10.2.89-1 [164 MB]\n",
            "Get:35 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-curand-10-2 10.2.89-1 [38.9 MB]\n",
            "Get:36 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-curand-dev-10-2 10.2.89-1 [39.1 MB]\n",
            "Get:37 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-cusparse-10-2 10.2.89-1 [59.2 MB]\n",
            "Get:38 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-cusparse-dev-10-2 10.2.89-1 [59.7 MB]\n",
            "Get:39 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-npp-10-2 10.2.89-1 [56.7 MB]\n",
            "Get:40 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-npp-dev-10-2 10.2.89-1 [57.6 MB]\n",
            "Get:41 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nvml-dev-10-2 10.2.89-1 [53.8 kB]\n",
            "Get:42 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nvjpeg-10-2 10.2.89-1 [1,274 kB]\n",
            "Get:43 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nvjpeg-dev-10-2 10.2.89-1 [1,213 kB]\n",
            "Get:44 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nsight-compute-10-2 10.2.89-1 [3,730 B]\n",
            "Get:45 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nsight-systems-10-2 10.2.89-1 [3,146 B]\n",
            "Get:46 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nvgraph-10-2 10.2.89-1 [44.5 MB]\n",
            "Get:47 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-nvgraph-dev-10-2 10.2.89-1 [35.2 MB]\n",
            "Get:48 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-visual-tools-10-2 10.2.89-1 [389 MB]\n",
            "Get:49 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-tools-10-2 10.2.89-1 [2,512 B]\n",
            "Get:50 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-samples-10-2 10.2.89-1 [65.6 MB]\n",
            "Get:51 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-documentation-10-2 10.2.89-1 [54.1 MB]\n",
            "Get:52 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-libraries-dev-10-2 10.2.89-1 [2,630 B]\n",
            "Get:53 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-libraries-10-2 10.2.89-1 [2,600 B]\n",
            "Get:54 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-toolkit-10-2 10.2.89-1 [2,846 B]\n",
            "Get:55 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-runtime-10-2 10.2.89-1 [2,548 B]\n",
            "Get:56 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-demo-suite-10-2 10.2.89-1 [3,880 kB]\n",
            "Get:57 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-10-2 10.2.89-1 [2,574 B]\n",
            "Fetched 1,430 MB in 28s (51.5 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 57.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package cuda-license-10-2.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-license-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-license-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-10-2.\n",
            "Preparing to unpack .../01-cuda-misc-headers-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvcc-10-2.\n",
            "Preparing to unpack .../02-cuda-nvcc-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvcc-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cuobjdump-10-2.\n",
            "Preparing to unpack .../03-cuda-cuobjdump-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cuobjdump-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvprune-10-2.\n",
            "Preparing to unpack .../04-cuda-nvprune-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvprune-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-compiler-10-2.\n",
            "Preparing to unpack .../05-cuda-compiler-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-compiler-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvdisasm-10-2.\n",
            "Preparing to unpack .../06-cuda-nvdisasm-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvdisasm-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-gdb-10-2.\n",
            "Preparing to unpack .../07-cuda-gdb-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-gdb-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvprof-10-2.\n",
            "Preparing to unpack .../08-cuda-nvprof-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvprof-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-sanitizer-api-10-2.\n",
            "Preparing to unpack .../09-cuda-sanitizer-api-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-sanitizer-api-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-memcheck-10-2.\n",
            "Preparing to unpack .../10-cuda-memcheck-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-memcheck-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cudart-10-2.\n",
            "Preparing to unpack .../11-cuda-cudart-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-10-2.\n",
            "Preparing to unpack .../12-cuda-driver-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-10-2.\n",
            "Preparing to unpack .../13-cuda-cudart-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cupti-10-2.\n",
            "Preparing to unpack .../14-cuda-cupti-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cupti-dev-10-2.\n",
            "Preparing to unpack .../15-cuda-cupti-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvtx-10-2.\n",
            "Preparing to unpack .../16-cuda-nvtx-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvtx-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-10-2.\n",
            "Preparing to unpack .../17-cuda-command-line-tools-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nsight-10-2.\n",
            "Preparing to unpack .../18-cuda-nsight-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvvp-10-2.\n",
            "Preparing to unpack .../19-cuda-nvvp-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvvp-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-10-2.\n",
            "Preparing to unpack .../20-cuda-nvrtc-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-10-2.\n",
            "Preparing to unpack .../21-cuda-nvrtc-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-10-2.\n",
            "Preparing to unpack .../22-cuda-cusolver-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-10-2.\n",
            "Preparing to unpack .../23-cuda-cusolver-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package libcublas10.\n",
            "Preparing to unpack .../24-libcublas10_10.2.3.254-1_amd64.deb ...\n",
            "Unpacking libcublas10 (10.2.3.254-1) ...\n",
            "Selecting previously unselected package libcublas-dev.\n",
            "Preparing to unpack .../25-libcublas-dev_10.2.3.254-1_amd64.deb ...\n",
            "Unpacking libcublas-dev (10.2.3.254-1) ...\n",
            "Selecting previously unselected package cuda-cufft-10-2.\n",
            "Preparing to unpack .../26-cuda-cufft-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-10-2.\n",
            "Preparing to unpack .../27-cuda-cufft-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-curand-10-2.\n",
            "Preparing to unpack .../28-cuda-curand-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-curand-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-10-2.\n",
            "Preparing to unpack .../29-cuda-curand-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-10-2.\n",
            "Preparing to unpack .../30-cuda-cusparse-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-10-2.\n",
            "Preparing to unpack .../31-cuda-cusparse-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-npp-10-2.\n",
            "Preparing to unpack .../32-cuda-npp-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-npp-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-10-2.\n",
            "Preparing to unpack .../33-cuda-npp-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-10-2.\n",
            "Preparing to unpack .../34-cuda-nvml-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvjpeg-10-2.\n",
            "Preparing to unpack .../35-cuda-nvjpeg-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvjpeg-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvjpeg-dev-10-2.\n",
            "Preparing to unpack .../36-cuda-nvjpeg-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvjpeg-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nsight-compute-10-2.\n",
            "Preparing to unpack .../37-cuda-nsight-compute-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-compute-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nsight-systems-10-2.\n",
            "Preparing to unpack .../38-cuda-nsight-systems-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-systems-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-10-2.\n",
            "Preparing to unpack .../39-cuda-nvgraph-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-10-2.\n",
            "Preparing to unpack .../40-cuda-nvgraph-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-10-2.\n",
            "Preparing to unpack .../41-cuda-visual-tools-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-tools-10-2.\n",
            "Preparing to unpack .../42-cuda-tools-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-tools-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "Preparing to unpack .../43-freeglut3_2.8.1-3_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-3) ...\n",
            "Selecting previously unselected package freeglut3-dev:amd64.\n",
            "Preparing to unpack .../44-freeglut3-dev_2.8.1-3_amd64.deb ...\n",
            "Unpacking freeglut3-dev:amd64 (2.8.1-3) ...\n",
            "Selecting previously unselected package libxmu-headers.\n",
            "Preparing to unpack .../45-libxmu-headers_2%3a1.1.2-2_all.deb ...\n",
            "Unpacking libxmu-headers (2:1.1.2-2) ...\n",
            "Selecting previously unselected package libxmu-dev:amd64.\n",
            "Preparing to unpack .../46-libxmu-dev_2%3a1.1.2-2_amd64.deb ...\n",
            "Unpacking libxmu-dev:amd64 (2:1.1.2-2) ...\n",
            "Selecting previously unselected package x11proto-input-dev.\n",
            "Preparing to unpack .../47-x11proto-input-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-input-dev (2018.4-4) ...\n",
            "Selecting previously unselected package libxi-dev:amd64.\n",
            "Preparing to unpack .../48-libxi-dev_2%3a1.7.9-1_amd64.deb ...\n",
            "Unpacking libxi-dev:amd64 (2:1.7.9-1) ...\n",
            "Selecting previously unselected package cuda-samples-10-2.\n",
            "Preparing to unpack .../49-cuda-samples-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-samples-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-documentation-10-2.\n",
            "Preparing to unpack .../50-cuda-documentation-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-10-2.\n",
            "Preparing to unpack .../51-cuda-libraries-dev-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-libraries-10-2.\n",
            "Preparing to unpack .../52-cuda-libraries-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-10-2.\n",
            "Preparing to unpack .../53-cuda-toolkit-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-runtime-10-2.\n",
            "Preparing to unpack .../54-cuda-runtime-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-10-2.\n",
            "Preparing to unpack .../55-cuda-demo-suite-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-10-2 (10.2.89-1) ...\n",
            "Selecting previously unselected package cuda-10-2.\n",
            "Preparing to unpack .../56-cuda-10-2_10.2.89-1_amd64.deb ...\n",
            "Unpacking cuda-10-2 (10.2.89-1) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-3) ...\n",
            "Setting up libxmu-headers (2:1.1.2-2) ...\n",
            "Setting up cuda-license-10-2 (10.2.89-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-10.2/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up cuda-nvgraph-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvprune-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvrtc-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvtx-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvjpeg-10-2 (10.2.89-1) ...\n",
            "Setting up libcublas10 (10.2.3.254-1) ...\n",
            "Setting up libcublas-dev (10.2.3.254-1) ...\n",
            "Setting up cuda-cufft-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nsight-compute-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cusparse-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cuobjdump-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-sanitizer-api-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvjpeg-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cusolver-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-misc-headers-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvvp-10-2 (10.2.89-1) ...\n",
            "Setting up freeglut3-dev:amd64 (2.8.1-3) ...\n",
            "Setting up cuda-curand-10-2 (10.2.89-1) ...\n",
            "Setting up x11proto-input-dev (2018.4-4) ...\n",
            "Setting up cuda-cudart-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-npp-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cufft-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-libraries-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-memcheck-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvrtc-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-driver-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-npp-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nsight-systems-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nsight-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvdisasm-10-2 (10.2.89-1) ...\n",
            "Setting up libxmu-dev:amd64 (2:1.1.2-2) ...\n",
            "Setting up cuda-nvml-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvgraph-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvcc-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-nvprof-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cusparse-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-compiler-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-runtime-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-curand-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cusolver-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-demo-suite-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-gdb-10-2 (10.2.89-1) ...\n",
            "Setting up libxi-dev:amd64 (2:1.7.9-1) ...\n",
            "Setting up cuda-cudart-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-libraries-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-visual-tools-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-samples-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cupti-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-documentation-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-cupti-dev-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-command-line-tools-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-tools-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-toolkit-10-2 (10.2.89-1) ...\n",
            "Setting up cuda-10-2 (10.2.89-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list:53 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!sudo apt upgrade -y\n",
        "!mkdir install \n",
        "%cd install\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
        "!sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
        "!sudo add-apt-repository \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/ /\"\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda-10-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ehe40OJKSLNk",
        "outputId": "f3e3099b-722b-4e91-cc8e-dd90a9b02e64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Wed_Oct_23_19:24:38_PDT_2019\n",
            "Cuda compilation tools, release 10.2, V10.2.89\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE7QcgfWtkqa"
      },
      "source": [
        "# source"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQtcxg8Tz4-u"
      },
      "source": [
        "### import packet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc1BTagiz7aF",
        "outputId": "24fd22a7-bc97-4354-f3d0-e478ce4b7080"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import scipy.sparse as sp\n",
        "import sys\n",
        "import dgl\n",
        "from torch.nn import Module, Dropout\n",
        "from dgl.nn.pytorch import GraphConv\n",
        "from torch.nn.functional import relu, softmax\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.nn import Module, Linear\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import scipy.sparse as sp\n",
        "from math import log\n",
        "from sklearn import svm\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.spatial.distance import cosine\n",
        "from torch.utils.data import DataLoader\n",
        "from random import shuffle\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import collections\n",
        "import math\n",
        "from scipy.sparse import coo_matrix\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn.functional import nll_loss,cross_entropy\n",
        "import torch\n",
        "from numpy import mean\n",
        "from torch import log\n",
        "from torch.nn.functional import softmax\n",
        "from torch.optim import lr_scheduler,AdamW\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80OLAh0reCJ-"
      },
      "source": [
        "### set data_set name "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8MM13xu2eEzW"
      },
      "outputs": [],
      "source": [
        "# Vion VSMEC   ViCTSD  VSMEC_6\n",
        "dataet_name = 'VSMEC_6'\n",
        "f1_type = 'macro'\n",
        "if dataet_name == 'VSMEC' or dataet_name == 'ViCTSD':\n",
        "  f1_type = 'macro'\n",
        "elif dataet_name == 'Vion' :\n",
        "  f1_type = 'weighted'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOpewssfyyRi"
      },
      "source": [
        "### util "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PsscSuMeywQL"
      },
      "outputs": [],
      "source": [
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "\n",
        "def get_file(choice):\n",
        "    file_dict = {\"R8\": 'R8.txt', 'ohsumed': 'ohsumed.txt', '20news': \"20ng.txt\", 'mr': 'mr.txt','SVMC': 'SVMC.txt'}\n",
        "    if choice not in file_dict.keys():\n",
        "        raise FileNotFoundError\n",
        "    return file_dict[choice]\n",
        "\n",
        "\n",
        "def clean_str(string):\n",
        "    # string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    # string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    # string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    # string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    # string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    # string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    # string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    # string = re.sub(r\",\", \" , \", string)\n",
        "    # string = re.sub(r\"!\", \" ! \", string)\n",
        "    # string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    # string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    # string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    # string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "def setup_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "def parse_index_file(filename):\n",
        "    \"\"\"Parse index file.\"\"\"\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "\n",
        "\n",
        "def sample_mask(idx, l):\n",
        "    \"\"\"Create mask.\"\"\"\n",
        "    mask = np.zeros(l)\n",
        "    mask[idx] = 1\n",
        "    return np.array(mask, dtype=np.bool)\n",
        "\n",
        "\n",
        "def load_corpus(dataset_str):\n",
        "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(\"/content/data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    x, y, tx, ty, allx, ally, adj = tuple(objects)\n",
        "    print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "    labels = np.vstack((ally, ty))\n",
        "    print(len(labels))\n",
        "\n",
        "    train_idx_orig = parse_index_file(\n",
        "        \"/content/data/{}.train.index\".format(dataset_str))\n",
        "    train_size = len(train_idx_orig)\n",
        "\n",
        "    val_size = train_size - x.shape[0]\n",
        "    test_size = tx.shape[0]\n",
        "\n",
        "    idx_train = range(len(y))\n",
        "    idx_val = range(len(y), len(y) + val_size)\n",
        "    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n",
        "\n",
        "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
        "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
        "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
        "\n",
        "    y_train = np.zeros(labels.shape)\n",
        "    y_val = np.zeros(labels.shape)\n",
        "    y_test = np.zeros(labels.shape)\n",
        "    y_train[train_mask, :] = labels[train_mask, :]\n",
        "    y_val[val_mask, :] = labels[val_mask, :]\n",
        "    y_test[test_mask, :] = labels[test_mask, :]\n",
        "\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size\n",
        "setup_seed(9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjon6T-OtmPb"
      },
      "source": [
        "## model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OP_WL-h7tlUz"
      },
      "outputs": [],
      "source": [
        "class finetunedphoBert(Module):\n",
        "    def __init__(self, class_num):\n",
        "        super().__init__()\n",
        "        #bert-base-uncased\n",
        "        # self.model = RobertaModel.from_pretrained('/content/PhoBERT_base_fairseq', checkpoint_file='model.pt', bpe='fastbpe', bpe_codes='/content/PhoBERT_base_fairseq/bpe.codes').eval()\n",
        "        self.model = AutoModel.from_pretrained('vinai/phobert-base')\n",
        "        # self.model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "        self.linear = Linear(768, class_num)\n",
        "\n",
        "    def forward(self, input):\n",
        "        predict = self.model(input).last_hidden_state[:,0,:]\n",
        "        return self.linear(predict)\n",
        "\n",
        "class GCN(Module):\n",
        "    def __init__(self, class_num):\n",
        "        super().__init__()\n",
        "        self.Conv1 = GraphConv(768, 200, weight=True, activation=relu)\n",
        "        self.Conv2 = GraphConv(200, class_num, weight=True)\n",
        "        self.dropout=Dropout(0.1)\n",
        "        self.dropout2=Dropout(0.2)\n",
        "\n",
        "\n",
        "    def forward(self, graph: dgl.graph, feature: torch.tensor):\n",
        "        predict1 = self.dropout(self.Conv1(graph, feature, edge_weight=graph.edata['w']))\n",
        "        predict2 = self.dropout2(self.Conv2(graph, predict1, edge_weight=graph.edata['w']))\n",
        "        return predict2\n",
        "\n",
        "\n",
        "class phoBertGCN(Module):\n",
        "    def __init__(self, pretrained_path, label_size, lam=0.3):\n",
        "        \"\"\"\n",
        "        :param pretrained_path: path to pretrained bert model\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.phoBertModel = finetunedphoBert(label_size)\n",
        "        self.phoBertModel.load_state_dict(torch.load(pretrained_path))\n",
        "        self.gcn = GCN(label_size)\n",
        "        self.lam = lam\n",
        "\n",
        "    def forward(self, sentences, features,attention, graph, indexs):\n",
        "        last_predict = self.phoBertModel.model(sentences,attention_mask=attention).last_hidden_state[:, 0, :]\n",
        "        features[indexs] = last_predict.detach()\n",
        "        gcn_predict = self.gcn(graph, features)\n",
        "        bert_predict = self.phoBertModel.linear(last_predict)\n",
        "        predict = softmax(gcn_predict[indexs], -1) * self.lam + (1 - self.lam) * softmax(bert_predict, -1)\n",
        "        return predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47A_nGouvJVi"
      },
      "source": [
        "## buildgraph for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRmMoPSdvKs8",
        "outputId": "83a4886a-2294-4978-d9c3-1058647cb9fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799, 5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869, 5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879, 5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909, 5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929, 5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029, 6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079, 6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089, 6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099, 6100, 6101, 6102, 6103, 6104, 6105, 6106, 6107, 6108, 6109, 6110, 6111, 6112, 6113, 6114, 6115, 6116, 6117, 6118, 6119, 6120, 6121, 6122, 6123, 6124, 6125, 6126, 6127, 6128, 6129, 6130, 6131, 6132, 6133, 6134, 6135, 6136, 6137, 6138, 6139, 6140, 6141, 6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149, 6150, 6151, 6152, 6153, 6154, 6155, 6156, 6157, 6158, 6159, 6160, 6161, 6162, 6163, 6164, 6165, 6166, 6167, 6168, 6169, 6170, 6171, 6172, 6173, 6174, 6175, 6176, 6177, 6178, 6179, 6180, 6181, 6182, 6183, 6184, 6185, 6186, 6187, 6188, 6189, 6190, 6191, 6192, 6193, 6194, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 6202, 6203, 6204, 6205, 6206, 6207, 6208, 6209, 6210, 6211, 6212, 6213, 6214, 6215, 6216, 6217, 6218, 6219, 6220, 6221, 6222, 6223, 6224, 6225, 6226, 6227, 6228, 6229, 6230, 6231, 6232, 6233, 6234, 6235, 6236, 6237, 6238, 6239, 6240]\n",
            "[3072, 3918, 4466, 1212, 615, 4756, 800, 4083, 3385, 465, 3060, 4853, 2027, 1576, 1423, 5230, 3819, 372, 3285, 1118, 1913, 2257, 201, 2470, 558, 2477, 4824, 3211, 4190, 3354, 2242, 3375, 1358, 3450, 1770, 1522, 2854, 2554, 5362, 1535, 4653, 2288, 4552, 1036, 3029, 3712, 2119, 1914, 4264, 3208, 4655, 4513, 5031, 4220, 2774, 4966, 1944, 2911, 2346, 1606, 4076, 3781, 2770, 4635, 4056, 3428, 1802, 4393, 797, 629, 4080, 4514, 3627, 5015, 4294, 3330, 1361, 1969, 4059, 2903, 250, 1073, 5402, 1776, 3168, 3528, 2530, 3301, 1664, 2053, 4410, 1839, 5345, 3532, 155, 3941, 5391, 753, 2158, 4134, 1789, 1533, 2900, 1026, 2191, 3467, 894, 4085, 796, 2873, 5121, 2109, 1676, 3421, 1329, 278, 2273, 5282, 841, 5493, 3157, 3382, 2043, 358, 4435, 669, 1099, 2539, 2884, 2095, 3921, 2605, 817, 1769, 2710, 4745, 5066, 5543, 1273, 4585, 5368, 4130, 1259, 2026, 2799, 4620, 3991, 2617, 5538, 2493, 3309, 5081, 919, 4935, 1054, 4426, 497, 1648, 3562, 1414, 884, 4382, 4047, 2034, 2296, 719, 4327, 745, 105, 3813, 2915, 4864, 2807, 5480, 321, 5384, 1389, 2504, 5059, 1887, 3343, 2733, 1755, 259, 4931, 2018, 430, 4009, 4406, 4780, 304, 5063, 3899, 1592, 4957, 5071, 3833, 2489, 3541, 2901, 2170, 732, 2372, 3783, 367, 3075, 2219, 3924, 522, 191, 3944, 2309, 1741, 1957, 309, 5097, 4873, 828, 1783, 1116, 4132, 4368, 2815, 2819, 5180, 4763, 2490, 252, 2549, 2695, 2664, 5088, 2427, 444, 1738, 3327, 5192, 5019, 3235, 4869, 5338, 1169, 2801, 2353, 4921, 5129, 290, 4982, 2214, 1285, 1409, 2562, 2672, 1049, 1216, 3465, 172, 3816, 32, 4373, 4830, 1479, 4692, 2671, 5190, 4652, 1061, 3634, 4260, 1064, 1200, 3312, 5404, 2169, 4045, 4768, 4079, 159, 1055, 4042, 5046, 5335, 38, 1342, 4648, 3886, 1095, 1424, 5295, 4543, 3748, 1854, 2361, 2429, 4542, 1041, 2619, 20, 1727, 3367, 113, 3174, 262, 4154, 4729, 528, 3488, 5245, 4123, 1478, 2408, 5324, 180, 5426, 3874, 935, 3362, 3510, 5430, 3969, 5205, 326, 1589, 1295, 950, 1980, 5200, 936, 2052, 34, 4551, 4127, 1336, 4464, 4129, 2941, 2007, 1343, 5358, 4492, 1204, 293, 3828, 4409, 642, 4673, 910, 5138, 2358, 4228, 916, 2536, 756, 4458, 1077, 1313, 3111, 1085, 1449, 4948, 3349, 4927, 4043, 3996, 3145, 4969, 2805, 1620, 4929, 2676, 2104, 1951, 3945, 4813, 3775, 825, 5290, 5260, 4520, 5193, 54, 336, 4804, 1369, 3834, 306, 2956, 3887, 1037, 1511, 3148, 3156, 3427, 899, 5164, 4162, 3618, 1020, 1838, 860, 317, 2276, 3498, 723, 2056, 3818, 934, 5305, 5243, 1470, 4198, 3735, 4882, 3329, 1291, 740, 4597, 3393, 588, 3658, 2213, 1402, 3117, 639, 4950, 2298, 2037, 19, 3654, 333, 461, 2624, 4698, 1105, 446, 5397, 1122, 2981, 351, 3731, 4979, 2559, 4177, 2336, 1205, 5075, 80, 5228, 2395, 193, 1675, 1186, 1441, 292, 3475, 4861, 561, 4600, 382, 4353, 5172, 1668, 4754, 1048, 2186, 5483, 1381, 2623, 5052, 3347, 5219, 4418, 2806, 3681, 380, 39, 1062, 889, 4512, 5443, 1909, 1510, 815, 3143, 4394, 3951, 1587, 4900, 4014, 3107, 1484, 8, 1168, 4726, 4204, 1191, 1515, 5506, 807, 4846, 5011, 3476, 255, 3762, 4708, 1817, 2487, 3804, 2537, 5073, 1895, 3009, 1590, 3416, 5354, 1334, 4886, 1565, 1594, 4800, 751, 3905, 3773, 3865, 4897, 2466, 4135, 638, 886, 2337, 5188, 2616, 4206, 5360, 2682, 1883, 2890, 4384, 814, 1207, 1560, 439, 3911, 4781, 632, 4596, 5331, 1362, 2515, 1028, 1640, 1684, 5130, 1609, 2106, 2751, 1297, 4424, 1567, 3414, 4432, 5165, 3956, 1407, 2953, 3975, 2717, 1157, 4791, 4491, 1614, 3537, 3495, 3982, 521, 2005, 3507, 2830, 5288, 327, 3426, 4292, 125, 1263, 5037, 1633, 2091, 3909, 2387, 1636, 1541, 4793, 1017, 4775, 4152, 793, 5195, 1840, 862, 5231, 1096, 2192, 3561, 888, 1235, 1996, 4507, 933, 4289, 4913, 3643, 829, 3960, 5236, 95, 2133, 3178, 4343, 1652, 868, 203, 2899, 1604, 2958, 3647, 4097, 1151, 4725, 1982, 4335, 656, 4070, 376, 5053, 2147, 1058, 4792, 2802, 3141, 2003, 185, 980, 4446, 4539, 2200, 4691, 1312, 1765, 1325, 2622, 606, 4669, 2610, 1198, 3377, 3931, 4362, 2258, 5535, 805, 4269, 409, 3277, 1233, 2314, 1448, 1699, 1208, 4942, 3485, 3258, 5083, 5270, 2510, 3745, 2839, 4401, 1250, 626, 3994, 3480, 3614, 2727, 924, 4207, 3372, 2909, 4849, 1528, 5467, 675, 2207, 132, 4349, 4553, 1141, 3445, 2175, 3525, 1573, 4099, 4374, 2652, 2077, 4733, 5403, 1247, 1942, 137, 3649, 3191, 4282, 1779, 609, 1735, 4924, 4298, 785, 2360, 1089, 1721, 1513, 5217, 2347, 3835, 1303, 2476, 5427, 4696, 2315, 3737, 3544, 2855, 554, 4383, 672, 2985, 5242, 4837, 3547, 1429, 4216, 5372, 4579, 658, 2099, 341, 3626, 1875, 3558, 4311, 3241, 4501, 4980, 4157, 3870, 5029, 311, 1365, 929, 876, 4991, 4508, 3919, 3120, 5469, 3486, 1886, 799, 1918, 30, 4302, 5365, 2083, 4150, 5509, 2853, 3001, 4688, 2062, 4161, 1412, 5183, 3942, 3597, 5393, 1845, 5048, 3109, 136, 5380, 1431, 4616, 1184, 75, 849, 4622, 4720, 4486, 5025, 1355, 2425, 1823, 2786, 3795, 4515, 224, 3788, 5268, 69, 4241, 663, 1154, 596, 4450, 2974, 2556, 4461, 3936, 3369, 4066, 576, 4868, 5276, 1694, 4075, 4485, 1714, 3181, 2292, 154, 5392, 2916, 5262, 3997, 3052, 1973, 1350, 4946, 320, 1121, 1149, 5189, 1563, 2571, 101, 1436, 3400, 3987, 449, 436, 4586, 1844, 24, 3608, 1632, 460, 1888, 2681, 4407, 2996, 790, 1743, 3452, 36, 4431, 1305, 2196, 5460, 3578, 249, 3002, 3901, 750, 1865, 4429, 4983, 2856, 2144, 1293, 3576, 4959, 577, 2534, 357, 3246, 5457, 3366, 631, 2817, 2698, 3149, 2139, 3047, 2141, 4533, 3595, 3023, 4943, 1, 2046, 2224, 821, 1228, 1030, 4764, 5527, 3170, 913, 3061, 1638, 5429, 1693, 3638, 4903, 2935, 5186, 1771, 838, 3336, 340, 4573, 1150, 5314, 1963, 5140, 2726, 37, 4139, 4940, 2596, 2440, 2631, 3632, 412, 2156, 1952, 2423, 3101, 493, 831, 5013, 5406, 1849, 3973, 1483, 4560, 3974, 809, 822, 1282, 4639, 3663, 2740, 4682, 2271, 1621, 1931, 3302, 5277, 3386, 3607, 4048, 5315, 539, 2042, 2236, 4839, 644, 1876, 354, 1469, 2501, 4428, 5167, 378, 1072, 3127, 3927, 3288, 3262, 952, 421, 5002, 1256, 2966, 3671, 13, 837, 4475, 3800, 4734, 4465, 4535, 646, 1438, 4703, 1856, 2263, 257, 2793, 2396, 2810, 3639, 3932, 4257, 5258, 5369, 998, 3214, 3590, 275, 1723, 1953, 1975, 2834, 3577, 5463, 4523, 3420, 1375, 4184, 3820, 4623, 5042, 1933, 4093, 3600, 2343, 861, 3601, 1734, 3549, 4376, 3090, 5149, 389, 2560, 4732, 2201, 2800, 4245, 3825, 4667, 12, 5504, 3988, 3759, 4346, 1897, 1920, 1958, 2583, 3005, 511, 3836, 3279, 5522, 2030, 842, 4544, 324, 3196, 2709, 463, 3665, 2959, 4420, 3403, 686, 863, 162, 2898, 3871, 2637, 3650, 2076, 4934, 2517, 568, 2788, 997, 1446, 1015, 1202, 5515, 387, 1990, 2375, 1552, 3135, 3116, 3158, 4815, 795, 3074, 5154, 3512, 1229, 264, 5347, 1733, 1371, 2683, 2294, 2454, 246, 1927, 4060, 1106, 3070, 4782, 5257, 79, 567, 2743, 627, 1955, 2195, 5292, 4993, 170, 2655, 5229, 4587, 3150, 3727, 3686, 4738, 2508, 3552, 2722, 4605, 5286, 3790, 3359, 4308, 3553, 3268, 501, 5320, 5201, 3039, 3554, 2713, 4372, 207, 1607, 2166, 1902, 4008, 4599, 5118, 1908, 2582, 4318, 231, 4646, 3885, 3674, 3212, 5363, 99, 4487, 3163, 313, 3995, 1185, 2868, 1403, 5420, 5018, 1600, 4490, 5173, 3350, 2832, 1213, 2645, 366, 4084, 2039, 2729, 2451, 1180, 3084, 956, 1420, 1159, 852, 1916, 3166, 5327, 1495, 3098, 5275, 1271, 5361, 2742, 3468, 3500, 3730, 373, 5438, 575, 4774, 1766, 1860, 25, 5054, 995, 2123, 171, 3557, 619, 3007, 4778, 4789, 2435, 3289, 496, 338, 4707, 4797, 4496, 6, 3469, 1243, 1843, 1378, 4918, 3456, 2833, 3984, 3862, 3079, 1542, 2881, 2167, 2980, 1351, 4604, 1814, 2813, 734, 1193, 258, 5170, 4737, 4017, 4670, 4408, 895, 4281, 4091, 5225, 3161, 4710, 298, 525, 422, 2925, 4001, 914, 2416, 896, 2928, 4867, 5078, 2621, 3881, 1921, 58, 4377, 5456, 3977, 5203, 5047, 1665, 925, 5390, 4273, 1790, 1490, 2067, 657, 882, 4171, 2880, 3636, 3711, 4324, 256, 4499, 598, 717, 5376, 1517, 2792, 5127, 2146, 4411, 4325, 184, 4637, 2741, 225, 1655, 3739, 5133, 932, 3687, 2345, 500, 2578, 3064, 4941, 1759, 5074, 4977, 687, 2795, 2403, 730, 586, 1965, 3257, 3854, 5297, 4823, 4878, 3518, 2761, 840, 3778, 4651, 1658, 4746, 1024, 819, 787, 5174, 1043, 1315, 4355, 1111, 481, 890, 4333, 166, 3502, 945, 5168, 2607, 3787, 775, 2498, 1657, 2386, 3439, 2366, 4357, 3197, 3531, 2478, 2990, 597, 1804, 2092, 4248, 4871, 4235, 3387, 2014, 2779, 4247, 2495, 73, 2938, 3843, 4676, 4847, 5141, 2850, 2494, 2615, 4251, 2558, 5237, 390, 4713, 905, 5501, 2268, 1433, 1011, 2696, 1705, 623, 1067, 3496, 2370, 2072, 3893, 1457, 1199, 4992, 1742, 4571, 3761, 1426, 5466, 1161, 2035, 5302, 2399, 3073, 1923, 5296, 2070, 1756, 3900, 4140, 1142, 5214, 4128, 1004, 2524, 2232, 582, 5208, 1761, 2325, 591, 527, 2594, 680, 3415, 4865, 4504, 5419, 4755, 419, 5162, 4538, 3263, 1170, 1181, 5308, 307, 2210, 1698, 5107, 4840, 824, 3132, 1038, 2620, 3695, 1425, 2221, 4915, 3164, 1155, 3207, 3245, 1772, 5446, 4149, 4340, 2203, 1071, 3113, 3219, 3033, 109, 2787, 5318, 1546, 3655, 2650, 698, 472, 369, 2013, 4223, 1123, 3335, 272, 2797, 2194, 2481, 874, 131, 4961, 2507, 2442, 2136, 2665, 2240, 406, 447, 4185, 641, 3850, 1674, 302, 206, 1194, 4454, 877, 3419, 1601, 363, 1904, 2428, 5198, 2272, 4986, 3765, 81, 4672, 4631, 1700, 2584, 1696, 5204, 3019, 4574, 1308, 3215, 3750, 3721, 3, 988, 1053, 4660, 5226, 3291, 265, 4053, 3472, 4598, 3793, 429, 1642, 3448, 2421, 1292, 4888, 385, 788, 2545, 5316, 1786, 242, 1660, 3089, 1945, 1924, 4089, 978, 2355, 1376, 4369, 5166, 1986, 470, 1989, 1977, 2870, 3332, 1070, 4192, 5079, 4339, 1678, 4558, 2209, 3756, 4716, 5263, 4565, 3774, 1656, 4909, 5069, 296, 29, 1785, 4549, 3986, 2877, 991, 5038, 83, 570, 4663, 3406, 5274, 3989, 1066, 3233, 3391, 4562, 1608, 4518, 3484, 2684, 78, 3832, 979, 2412, 5366, 3085, 3364, 3928, 4470, 2669, 3318, 3559, 959, 4690, 4516, 3438, 5474, 3693, 4256, 1463, 2059, 1749, 2544, 5084, 3844, 4267, 3504, 1523, 3902, 2644, 1707, 1319, 2753, 4832, 1943, 1787, 3546, 857, 1558, 4686, 3123, 4293, 612, 4252, 2542, 920, 942, 4923, 4844, 3527, 1613, 2846, 2390, 1631, 4879, 1820, 3716, 3585, 1650, 3278, 3034, 1059, 1940, 4537, 1564, 4711, 566, 783, 1411, 4975, 559, 1005, 2626, 1520, 946, 5094, 1136, 3394, 2837, 1794, 2177, 1472, 1456, 102, 898, 921, 117, 237, 2954, 5234, 3959, 1384, 2231, 3433, 2703, 153, 4910, 3353, 843, 4702, 4065, 96, 2663, 3030, 5045, 683, 2882, 1763, 5098, 2483, 2561, 864, 4258, 2447, 3875, 4650, 971, 4822, 1682, 1740, 673, 1868, 1822, 476, 1706, 1491, 3021, 283, 3644, 3173, 2590, 711, 4320, 2112, 798, 1948, 3889, 3690, 4580, 371, 4283, 4665, 3617, 3095, 4715, 4484, 5196, 1686, 4169, 289, 1818, 3305, 771, 301, 4852, 1029, 151, 1333, 2670, 1370, 5072, 2969, 3538, 696, 3493, 1022, 1364, 316, 2351, 149, 834, 5064, 2963, 2373, 2679, 3536, 1281, 90, 2050, 3017, 3437, 4999, 1647, 194, 3184, 1279, 3805, 189, 2465, 5373, 4875, 901, 853, 2080, 1691, 548, 2284, 1877, 4577, 3744, 1416, 3317, 4035, 1930, 1474, 4917, 4862, 1138, 5503, 1034, 1871, 4870, 964, 4831, 386, 2638, 5383, 4643, 5055, 2459, 1842, 4217, 1653, 4031, 1885, 510, 4522, 5525, 5010, 3352, 2780, 4534, 3656, 4594, 3736, 4000, 1075, 3265, 923, 3620, 715, 5470, 549, 3517, 3884, 2197, 4736, 1349, 3605, 2731, 4389, 2618, 3839, 200, 990, 107, 4019, 5096, 2084, 3979, 1158, 1747, 2687, 2826, 2279, 2426, 4809, 2608, 1206, 5254, 3602, 5266, 5016, 3379, 228, 4105, 1241, 5478, 1731, 1298, 3505, 14, 3086, 1427, 1580, 76, 4632, 3270, 2049, 17, 5044, 3521, 4976, 984, 3867, 982, 4640, 5537, 1145, 4119, 4677, 1101, 1310, 887, 3144, 4769, 1128, 3897, 1422, 1709, 4811, 2809, 1475, 3515, 2569, 3722, 553, 2215, 2379, 350, 4478, 3470, 5169, 3683, 4629, 4753, 2961, 892, 4661, 2694, 4884, 1712, 426, 2065, 1335, 2066, 1739, 4531, 4448, 1283, 4990, 464, 322, 4415, 2452, 4932, 3641, 435, 5012, 1960, 2163, 4820, 4898, 4312, 124, 437, 4578, 1746, 3791, 4747, 893, 4842, 4657, 655, 818, 581, 4872, 2061, 4359, 3680, 2527, 4741, 2418, 4816, 161, 931, 1597, 4576, 2448, 3345, 2848, 3535, 1156, 2114, 3591, 4284, 3069, 1098, 2148, 3236, 5153, 1507, 5028, 2152, 3370, 2970, 2732, 5310, 5461, 4994, 2317, 328, 391, 4203, 3361, 3972, 794, 546, 1531, 1353, 2036, 2226, 4020, 974, 3599, 424, 267, 634, 3838, 1280, 4904, 74, 1539, 1347, 3443, 129, 2509, 1760, 3651, 2011, 2535, 1232, 1534, 1673, 3616, 3628, 1625, 5181, 1172, 2902, 4253, 3784, 4771, 633, 3826, 5280, 2965, 3724, 5283, 3779, 2922, 764, 692, 2327, 587, 3622, 2389, 835, 4848, 1092, 941, 254, 3097, 677, 2553, 1724, 4219, 2262, 2398, 3038, 2648, 5453, 5090, 4970, 2755, 777, 1120, 1569, 2632, 3071, 3153, 2643, 3225, 1519, 4107, 1681, 2124, 2449, 827, 2485, 839, 3492, 4023, 4164, 1500, 1000, 441, 1496, 2068, 395, 4611, 706, 1688, 1836, 930, 4341, 3210, 1254, 2864, 1644, 737, 944, 2568, 211, 538, 4344, 4925, 118, 4108, 4798, 3861, 4183, 1363, 1035, 3395, 4783, 1516, 4371, 3954, 1461, 2878, 2657, 4417, 2044, 1773, 3717, 584, 2760, 3053, 3564, 1386, 2445, 2340, 5409, 4784, 2082, 977, 133, 736, 3619, 3218, 4404, 5212, 2910, 5298, 1827, 2164, 1873, 127, 1867, 1501, 2525, 3266, 1704, 1577, 3228, 229, 3992, 1864, 3713, 3961, 3298, 3964, 4546, 179, 1032, 2031, 452, 4776, 2071, 1396, 2714, 989, 856, 2929, 5432, 4396, 2305, 2492, 5027, 4413, 1057, 4765, 4141, 4613, 2394, 331, 5413, 176, 1269, 4717, 2704, 3673, 2411, 1278, 2943, 1322, 1550, 4166, 5067, 2430, 3824, 509, 513, 1988, 4367, 2948, 3311, 2225, 1505, 654, 4526, 557, 3898, 602, 2944, 4117, 2766, 3743, 1872, 4445, 4436, 56, 1471, 0, 2218, 3087, 5117, 2790, 5070, 178, 792, 5034, 2518, 3612, 325, 3314, 507, 1809, 4541, 2707, 3351, 4243, 2719, 2475, 4606, 636, 1874, 3877, 3953, 3980, 3891, 2252, 3726, 1044, 4109, 4025, 3049, 4313, 2097, 3358, 2150, 1603, 757, 5479, 1835, 2630, 1094, 3454, 3815, 5128, 4850, 1392, 468, 4072, 4414, 5041, 962, 505, 248, 2750, 5182, 5020, 4770, 4425, 1855, 4633, 222, 5339, 414, 614, 2474, 1042, 2089, 4036, 1340, 2550, 4859, 187, 2968, 175, 4350, 1251, 5108, 2291, 2160, 2814, 3283, 1437, 4905, 5238, 209, 3010, 2746, 3177, 3610, 1732, 1368, 1277, 3892, 2441, 3849, 879, 2641, 2836, 1464, 2778, 2280, 1112, 3910, 5329, 4170, 3652, 2318, 2521, 10, 2335, 27, 1309, 2666, 1805, 3682, 1330, 4156, 1164, 4358, 4930, 552, 2552, 5435, 2002, 712, 2926, 4827, 1110, 1238, 4956, 4952, 4936, 2803, 111, 1135, 2365, 135, 1476, 2342, 3749, 2057, 1078, 5100, 3092, 1715, 4566, 4295, 1781, 3506, 5340, 4796, 2984, 4336, 401, 2675, 197, 2143, 5060, 4590, 5239, 2455, 2950, 1466, 3119, 3962, 91, 261, 4271, 423, 5056, 873, 1797, 3763, 5447, 2241, 3024, 2942, 5507, 2055, 2502, 5400, 534, 5451, 637, 474, 2593, 488, 3138, 299, 1912, 961, 1373, 2702, 4683, 181, 4077, 520, 4250, 1132, 4687, 624, 1866, 4453, 2555, 1080, 5351, 4479, 1372, 4502, 4434, 3222, 3848, 755, 4480, 1728, 4641, 1115, 4442, 699, 4744, 3282, 738, 2341, 2546, 3308, 4229, 345, 4078, 3548, 4883, 2993, 716, 2155, 1976, 4706, 1998, 3516, 2312, 3594, 5223, 3978, 1338, 4375, 810, 330, 3514, 5502, 2939, 3574, 4002, 3708, 685, 4649, 2680, 1880, 1357, 4709, 3565, 221, 2919, 4209, 5539, 2246, 781, 9, 2840, 4735, 1195, 1133, 859, 2286, 3286, 2383, 3560, 455, 4026, 702, 3175, 1046, 2540, 975, 4928, 762, 1113, 1545, 1910, 2233, 2302, 3570, 508, 5261, 3151, 332, 1079, 4214, 970, 1850, 5291, 4175, 4563, 3398, 303, 595, 3946, 1274, 4524, 2111, 5087, 5422, 1596, 2324, 2235, 3938, 4998, 4433, 3837, 2079, 1503, 701, 5026, 2008, 3483, 999, 4278, 2012, 1430, 918, 3337, 4029, 483, 5161, 2484, 2278, 3732, 573, 2190, 3227, 5398, 4483, 3705, 1593, 3952, 1227, 2473, 5021, 958, 2270, 3042, 2444, 4444, 4274, 2228, 3635, 3131, 2991, 4536, 1224, 1554, 5364, 116, 2393, 1504, 3983, 1947, 4187, 1445, 4392, 4506, 3044, 1377, 4843, 3810, 4767, 2749, 1551, 3130, 4724, 492, 160, 3588, 4997, 450, 678, 383, 49, 3926, 3373, 2332, 3508, 5405, 4342, 5014, 2359, 2405, 5387, 2668, 4055, 855, 763, 3384, 110, 2595, 4277, 3642, 2917, 1460, 323, 3065, 1813, 5247, 4671, 689, 2193, 2237, 1869, 1799, 4208, 1341, 3965, 529, 1174, 714, 5233, 4155, 3100, 4148, 1219, 5142, 1753, 562, 5043, 2118, 5120, 1718, 2754, 4584, 659, 1367, 1917, 4112, 157, 3586, 5350, 5294, 4328, 4489, 3725, 4681, 2728, 2432, 4309, 2184, 5284, 89, 1585, 1758, 5065, 4920, 697, 5542, 2808, 5475, 2145, 2323, 3077, 2606, 284, 2115, 2446, 5526, 1163, 2604, 2872, 1833, 359, 1497, 2319, 4609, 4860, 3915, 1903, 1672, 1670, 1602, 4100, 3489, 1950, 4601, 1258, 4391, 1153, 992, 5411, 2032, 3584, 2249, 490, 4022, 2971, 746, 5524, 744, 2589, 2354, 620, 881, 2260, 4137, 5408, 1612, 3389, 3802, 902, 2406, 3958, 2420, 4106, 3328, 277, 5159, 377, 4570, 4714, 3933, 2048, 92, 3930, 1586, 3290, 1687, 1076, 3325, 1881, 3388, 4366, 3321, 4285, 4742, 5346, 3260, 2512, 2557, 5146, 5126, 177, 108, 4482, 1669, 4331, 4419, 779, 4314, 2998, 22, 286, 4010, 428, 2930, 5401, 2932, 707, 2313, 1579, 2469, 3823, 3188, 4225, 165, 3299, 1021, 1065, 3306, 1187, 4416, 4750, 4399, 2457, 2688, 3418, 2995, 5155, 1798, 4887, 5431, 3847, 1736, 2520, 3192, 4227, 2326, 2818, 2047, 3037, 1137, 276, 2127, 1236, 1033, 167, 986, 2075, 2769, 5251, 3169, 4213, 4337, 227, 146, 499, 1147, 1140, 263, 1182, 2063, 940, 4159, 1750, 342, 2871, 379, 2266, 728, 4833, 4825, 3907, 459, 953, 3556, 3147, 5433, 2538, 872, 1518, 1762, 4310, 2738, 3292, 5022, 3811, 2153, 2513, 3929, 204, 2128, 4322, 1752, 3869, 5528, 415, 608, 5227, 3478, 4619, 1173, 4807, 3410, 2829, 2931, 5476, 2439, 2182, 3701, 5157, 2526, 2879, 4299, 5143, 4772, 1270, 3320, 4996, 739, 5080, 2659, 5218, 3841, 1203, 60, 665, 1222, 4659, 4046, 84, 572, 5481, 1791, 5407, 86, 2, 1082, 3752, 4881, 5221, 398, 3985, 2674, 3903, 3957, 2434, 1027, 955, 735, 3108, 2250, 3099, 5371, 2362, 5139, 4799, 3653, 4412, 2178, 104, 2812, 4236, 5386, 4013, 5102, 238, 186, 3059, 3648, 4364, 2040, 2686, 2377, 2450, 506, 2285, 2725, 3193, 3604, 2599, 2159, 3827, 3068, 5210, 1780, 2921, 3274, 3754, 2134, 1851, 3530, 2723, 2565, 2885, 260, 28, 3522, 4268, 1624, 1188, 3402, 5008, 4063, 2330, 1591, 2861, 5160, 3934, 2783, 3078, 1848, 2137, 2693, 2239, 1144, 310, 4195, 2656, 4287, 2505, 4133, 3613, 885, 4722, 2176, 4519, 1667, 1230, 2004, 2777, 1013, 592, 2329, 2415, 4018, 5394, 5323, 5396, 3167, 3112, 1863, 4395, 347, 4427, 1117, 3859, 4786, 1447, 4801, 5144, 5423, 1979, 3971, 21, 3378, 2585, 1025, 471, 4452, 4627, 4004, 5091, 1899, 5455, 2283, 3720, 3678, 2597, 3429, 3424, 4173, 3520, 3234, 4007, 3542, 4914, 4762, 1559, 4116, 3096, 3519, 2023, 3014, 1801, 273, 652, 356, 3272, 600, 4181, 5541, 281, 1570, 2823, 1040, 2096, 5235, 2580, 705, 2773, 3963, 5468, 2756, 5095, 3685, 3000, 2983, 1012, 2888, 1171, 3016, 5061, 4345, 93, 5322, 3209, 3171, 3399, 3313, 5410, 236, 5332, 35, 4151, 1220, 1093, 5484, 4757, 4215, 4033, 5333, 1434, 1831, 3812, 4142, 4642, 190, 5465, 2500, 1276, 3692, 5109, 3259, 467, 4505, 3189, 5135, 4088, 485, 349, 1751, 1795, 4685, 2391, 5304, 4945, 1571, 1841, 3539, 866, 3125, 3407, 2021, 1299, 2825, 2845, 4877, 2122, 1637, 4397, 1253, 1992, 1455, 5249, 3999, 742, 130, 2295, 348, 2472, 3392, 232, 3458, 1889, 2563, 5030, 4061, 5342, 1974, 1018, 3243, 3334, 2811, 1056, 1878, 1966, 5378, 1651, 4662, 1246, 3976, 2289, 5222, 2306, 578, 3310, 3981, 1611, 1898, 5255, 4044, 1261, 4304, 2244, 457, 4122, 2401, 3771, 163, 5321, 1834, 4068, 1892, 202, 1462, 1296, 741, 555, 3563, 1453, 3700, 2364, 1395, 3459, 3543, 2025, 5444, 1710, 2090, 5215, 2979, 5179, 2918, 1421, 2051, 5103, 355, 4011, 2677, 2414, 3371, 2132, 5246, 867, 5530, 3409, 4385, 218, 5336, 700, 219, 2431, 2287, 3230, 4962, 5137, 3205, 2384, 3545, 2547, 1039, 2612, 3013, 1599, 2649, 2789, 3203, 4985, 4719, 3670, 4723, 2640, 2735, 516, 2087, 2886, 4323, 4160, 2874, 2117, 46, 1981, 3080, 1701, 2363, 199, 2174, 3581, 2202, 2081, 3740, 4588, 300, 4254, 2957, 1830, 4272, 682, 3894, 4916, 4834, 1068, 3661, 776, 4094, 2587, 233, 4511, 308, 1321, 2945, 3501, 4498, 48, 3134, 2862, 1852, 3027, 2851, 1997, 2064, 3630, 5412, 5136, 2368, 1509, 1458, 4828, 1307, 4067, 3672, 2768, 4120, 1408, 2248, 2045, 2344, 1905, 1435, 2041, 4944, 2436, 3728, 5519, 3840, 1468, 3688, 593, 2293, 4321, 3159, 2181, 4891, 4901, 173, 897, 3022, 4194, 2912, 2781, 1900, 3970, 1162, 4972, 4266, 1555, 1556, 1722, 2304, 370, 2706, 3950, 5259, 2533, 3201, 2700, 1211, 5206, 3766, 4477, 4550, 2161, 2310, 541, 3868, 5271, 2443, 297, 2913, 1143, 1009, 3864, 4338, 4618, 1406, 2689, 5450, 1443, 2920, 87, 1938, 5301, 5454, 854, 4300, 1339, 2497, 3423, 1547, 394, 1792, 3896, 3256, 2908, 650, 4196, 1695, 679, 1884, 2001, 4510, 2422, 1175, 2171, 4474, 3772, 4233, 2453, 3890, 772, 5312, 3943, 4180, 5486, 599, 640, 2522, 3677, 1890, 5076, 3355, 1690, 3948, 4679, 5050, 4947, 949, 1260, 5175, 928, 540, 42, 3411, 5518, 5379, 2404, 5086, 1968, 4006, 1659, 3043, 3769, 3451, 5512, 4495, 2889, 2308, 3785, 1306, 5532, 2496, 1109, 1301, 4802, 2960, 1999, 2162, 2866, 1561, 3694, 3623, 4153, 2471, 2999, 3276, 5148, 3280, 3333, 2627, 4030, 1617, 234, 1911, 7, 4301, 3718, 2274, 4906, 1465, 2413, 1237, 1521, 469, 2338, 1450, 3990, 5445, 907, 4817, 3925, 4390, 3457, 3244, 3057, 285, 4965, 1255, 139, 1717, 1870, 5185, 2316, 4826, 4034, 5495, 3878, 3273, 1858, 3357, 4178, 5459, 4125, 115, 1225, 782, 1197, 2006, 5093, 5253, 1487, 4126, 2782, 1380, 4899, 1626, 1578, 141, 4024, 2129, 2737, 1006, 4440, 1499, 3346, 5007, 1388, 5428, 3374, 4462, 4237, 2934, 2382, 826, 2678, 547, 1086, 4291, 2217, 1745, 4226, 2151, 5487, 2588, 5395, 4564, 2973, 556, 2230, 1031, 1962, 5442, 4158, 2757, 440, 3431, 3200, 5344, 3020, 5399, 947, 2816, 4270, 1390, 4081, 4751, 5248, 1793, 2350, 1993, 4548, 3464, 4244, 4530, 3529, 3295, 2894, 4527, 2073, 1016, 4039, 2189, 5536, 3155, 2827, 1907, 2634, 1964, 4602, 1454, 1777, 670, 4819, 2290, 1859, 1265, 2105, 3487, 3229, 3204, 4628, 4051, 2730, 2673, 4790, 5101, 268, 3615, 2574, 3054, 2085, 2869, 2205, 1266, 647, 4121, 2776, 288, 4145, 3247, 780, 388, 2142, 4087, 2661, 2060, 2937, 1936, 3006, 3300, 3998, 18, 1404, 3633, 2108, 2875, 5343, 4810, 271, 1125, 3251, 1284, 2828, 2299, 1486, 4103, 5289, 3704, 2179, 2017, 4517, 63, 1264, 3709, 1359, 2029, 5112, 560, 2923, 1788, 4727, 2720, 3046, 3770, 2883, 4319, 4568, 2374, 3853, 904, 3102, 346, 3914, 5151, 2972, 1119, 3430, 3028, 5125, 2570, 4016, 120, 3631, 5035, 2949, 5534, 454, 1987, 5194, 3220, 5191, 375, 4971, 943, 4532, 5082, 4275, 3789, 1418, 4037, 3857, 5545, 1286, 5264, 2461, 4114, 3845, 5418, 2600, 1492, 4748, 4168, 62, 2125, 1257, 5244, 479, 1508, 1615, 937, 1045, 4098, 4430, 4451, 869, 951, 2646, 315, 140, 2277, 339, 3226, 5171, 5436, 3114, 903, 617, 4829, 628, 2946, 1294, 4326, 4307, 2708, 2255, 4699, 145, 4329, 3462, 5485, 3360, 3540, 3920, 813, 3582, 3768, 4423, 2388, 4113, 4086, 4463, 2511, 2220, 1410, 1800, 3338, 2896, 4955, 5325, 1666, 2566, 4951, 2716, 2093, 3967, 2614, 4974, 565, 2349, 2433, 4555, 4894, 2301, 3105, 2038, 4040, 217, 5516, 1317, 3249, 4189, 3182, 2636, 1108, 1442, 967, 911, 4557, 2172, 720, 2204, 3580, 5494, 3817, 651, 1323, 3767, 364, 2952, 3206, 5033, 1543, 3659, 5269, 4593, 4497, 4808, 411, 2348, 3088, 2103, 804, 3213, 1356, 1348, 4381, 726, 2567, 1014, 362, 4902, 4437, 195, 504, 4880, 3831, 3879, 2685, 2514, 4199, 4743, 5122, 2978, 1857, 2238, 51, 4645, 2772, 1922, 1480, 4361, 3122, 1775, 1627, 3121, 2804, 5462, 2831, 3081, 3115, 912, 3842, 5039, 4678, 4443, 1824, 4664, 1925, 3091, 319, 2140, 3696, 1088, 3404, 4303, 3133, 4638, 648, 1583, 3589, 1394, 4471, 579, 2369, 721, 3592, 3413, 68, 2859, 4378, 4795, 1512, 3913, 3596, 3733, 3221, 3093, 3136, 4262, 963, 4032, 402, 878, 57, 4995, 1268, 4290, 3035, 4572, 1008, 3253, 3575, 3753, 4787, 3777, 4058, 2988, 4896, 3968, 1007, 1649, 3629, 4674, 758, 3401, 4960, 1646, 2251, 1959, 3198, 1130, 4334, 3128, 2745, 3003, 3287, 4607, 3912, 1366, 3183, 2467, 4230, 2269, 226, 4188, 519, 3186, 4261, 494, 1819, 3667, 2357, 708, 1047, 2876, 1582, 3160, 3094, 2183, 3237, 3435, 1697, 5207, 403, 969, 407, 1915, 4575, 1954, 1473, 2463, 2865, 360, 4447, 2628, 1318, 3422, 2962, 3646, 1451, 1972, 3460, 5317, 4493, 4232, 188, 1630, 2101, 5375, 3045, 2212, 2022, 1803, 2579, 1530, 489, 3066, 731, 3904, 3533, 5547, 972, 1146, 5434, 4356, 4193, 3474, 1023, 1244, 3640, 3040, 630, 5105, 2261, 3883, 486, 3526, 2653, 196, 4460, 3446, 3341, 4612, 5273, 5209, 3172, 4316, 1618, 2764, 759, 3271, 2528, 3691, 5523, 1737, 3760, 1538, 183, 4021, 1252, 2591, 33, 2867, 3363, 2229, 3202, 532, 4595, 2321, 147, 981, 1275, 3940, 1114, 4041, 2992, 3706, 400, 948, 1879, 3872, 3376, 247, 3568, 274, 4124, 2307, 4347, 1331, 3129, 4803, 266, 3866, 3645, 3830, 3199, 2468, 4926, 1967, 4380, 442, 4131, 5382, 3555, 2376, 1548, 724, 833, 3503, 5202, 4104, 4697, 1971, 3224, 1985, 544, 2135, 3194, 4179, 530, 3436, 3315, 1956, 1572, 253, 3434, 2629, 939, 2486, 4938, 291, 1189, 5496, 5471, 524, 2718, 2758, 3993, 410, 2611, 329, 3566, 2069, 4777, 4360, 3442, 1810, 4556, 2098, 5299, 4876, 4624, 4559, 927, 512, 3675, 85, 2265, 2860, 1671, 3534, 3267, 5184, 3684, 847, 1861, 368, 4240, 4438, 2491, 1929, 3217, 4912, 1536, 5176, 5417, 2977, 5452, 3579, 1595, 1354, 5491, 5328, 314, 1502, 3606, 4201, 2367, 725, 4259, 3232, 3319, 4138, 3281, 4092, 3326, 3473, 2763, 1393, 580, 1829, 3776, 2907, 5036, 2019, 1081, 4838, 3809, 3747, 4885, 4265, 5319, 361, 1778, 5057, 5110, 4626, 1231, 761, 2243, 134, 4521, 2759, 1398, 3242, 282, 768, 2253, 5326, 1432, 3447, 1192, 2222, 2697, 1828, 3322, 2893, 4111, 2784, 3550, 2838, 3676, 4012, 1019, 2639, 1553, 3082, 2000, 4503, 3110, 747, 2951, 601, 66, 2541, 3293, 604, 2437, 4987, 1262, 2016, 269, 5458, 3707, 4569, 5279, 846, 5482, 4895, 408, 5489, 3380, 1932, 3490, 1239, 1428, 3494, 5001, 45, 4617, 1214, 1210, 1846, 3780, 205, 1249, 1288, 543, 312, 3365, 4306, 1196, 5477, 3807, 4090, 1087, 2480, 2914, 3876, 3036, 4666, 4352, 4052, 517, 374, 4964, 1209, 5472, 2409, 122, 287, 3390, 3751, 1754, 754, 5517, 4218, 1768, 1978, 613, 3509, 245, 3076, 803, 1289, 168, 4005, 5330, 635, 4718, 1764, 2822, 4202, 3723, 4057, 1267, 3755, 1069, 2462, 4654, 2947, 2715, 871, 2843, 743, 2955, 3425, 4054, 2724, 2482, 2863, 4238, 1757, 65, 3863, 1581, 610, 2690, 5353, 4488, 1994, 535, 1311, 802, 994, 4610, 4806, 960, 144, 583, 3140, 2015, 3165, 2586, 5303, 733, 119, 213, 748, 2320, 198, 674, 3572, 5425, 4073, 4297, 1685, 215, 704, 5529, 1566, 1941, 1400, 445, 1961, 16, 2173, 1782, 4855, 458, 1327, 425, 957, 381, 2712, 668, 5077, 875, 397, 41, 4472, 4379, 417, 603, 112, 2592, 4212, 5388, 2020, 3139, 851, 3738, 2964, 3814, 251, 4191, 1796, 2548, 1382, 823, 922, 23, 660, 4874, 5348, 15, 4963, 2297, 4973, 2976, 1493, 1853, 3664, 5145, 1970, 2216, 432, 3146, 820, 2519, 502, 1815, 3216, 1896, 4592, 1716, 5106, 103, 1050, 4096, 1001, 2417, 1374, 4866, 5085, 1928, 2824, 1131, 1812, 830, 5505, 5540, 3657, 475, 622, 487, 1489, 2796, 4467, 3032, 3524, 5147, 4704, 1683, 3176, 1537, 5058, 2532, 4907, 3477, 551, 2371, 938, 3004, 2009, 5115, 3018, 4773, 3955, 3453, 2849, 515, 3187, 2897, 4761, 996, 4680, 1540, 2573, 128, 3966, 2794, 5005, 223, 4528, 2400, 2924, 3252, 5414, 4644, 405, 1477, 4589, 4730, 3882, 4908, 2028, 2821, 2407, 4231, 649, 3444, 909, 3323, 456, 2658, 858, 3513, 1703, 418, 3255, 2199, 4721, 1919, 3742, 2165, 393, 453, 4766, 3624, 5113, 2739, 152, 2967, 3397, 5306, 550, 4752, 392, 365, 571, 4684, 1248, 2887, 2576, 4210, 3238, 2397, 1784, 3254, 1744, 5089, 3611, 2259, 4101, 976, 2113, 4205, 2110, 574, 3008, 536, 1107, 4937, 778, 3808, 2744, 1532, 3063, 3482, 926, 3856, 1304, 3669, 1160, 1702, 3860, 503, 3625, 2223, 3851, 1610, 5311, 4049, 3666, 2603, 3786, 4854, 3758, 4625, 1166, 1584, 1074, 594, 2858, 40, 3497, 434, 210, 1452, 1946, 2157, 138, 5131, 1183, 5187, 2987, 334, 477, 1176, 2107, 5544, 4500, 806, 2936, 2247, 1514, 3764, 305, 2613, 1767, 666, 770, 760, 1677, 4200, 5440, 4509, 3571, 590, 4457, 2488, 2456, 1557, 427, 169, 3937, 50, 3275, 71, 1568, 5241, 1926, 148, 2765, 1344, 1629, 4933, 2892, 4279, 1352, 4582, 3935, 1882, 11, 965, 2311, 5114, 801, 3012, 61, 5049, 4348, 4863, 1165, 1234, 4144, 880, 5508, 1218, 3405, 4858, 3923, 2300, 3432, 1052, 3297, 589, 1482, 142, 1242, 4634, 1937, 216, 1002, 3316, 1316, 2187, 1692, 2402, 514, 3015, 1290, 2785, 2102, 3710, 53, 4456, 3668, 1806, 3011, 3801, 4621, 2975, 4, 5533, 883, 688, 4476, 5213, 1935, 2752, 1808, 1103, 2721, 4473, 3231, 3179, 865, 1003, 4095, 3729, 1097, 3126, 174, 1506, 1240, 5511, 2088, 722, 3304, 5024, 3821, 2206, 3342, 1588, 4402, 4614, 4038, 1605, 1639, 5040, 2130, 5062, 2054, 1300, 987, 789, 2180, 713, 1302, 1399, 3660, 438, 2381, 5004, 72, 2352, 766, 4841, 5281, 4700, 4893, 1346, 1713, 5134, 4182, 4919, 3083, 5381, 3699, 1906, 4851, 2503, 569, 5341, 3223, 491, 5313, 2094, 4608, 4954, 5032, 2667, 671, 3908, 5092, 2609, 2598, 1326, 1439, 2654, 3051, 4246, 3621, 4186, 2701, 55, 2256, 1391, 4561, 2168, 4668, 653, 4779, 3587, 5421, 100, 4172, 4731, 954, 4071, 4388, 3383, 4003, 1226, 4365, 4814, 1332, 3916, 607, 2762, 3339, 667, 5424, 3906, 4354, 5520, 478, 4989, 5521, 2572, 4143, 5267, 2328, 3873, 2499, 5357, 1488, 3449, 1179, 2523, 4296, 4315, 605, 1525, 5359, 5163, 3118, 836, 3609, 1645, 47, 4728, 4856, 973, 4249, 64, 2392, 1983, 243, 1383, 3858, 4788, 1598, 2120, 3195, 645, 3567, 2531, 5152, 5356, 5068, 2642, 968, 523, 1498, 4165, 3340, 533, 3067, 3703, 2989, 2126, 3381, 1825, 5293, 466, 850, 3471, 4363, 3573, 2356, 2380, 3746, 5352, 2564, 4469, 1320, 5278, 3348, 4224, 4222, 5252, 1575, 2982, 5150, 4694, 4163, 1272, 4689, 1984, 484, 1444, 3491, 3344, 769, 618, 4567, 2891, 2933, 3593, 2116, 158, 1725, 5285, 729, 5510, 1090, 5300, 4529, 1314, 4583, 2647, 2852, 3031, 2227, 3396, 5111, 94, 1847, 2419, 2543, 2086, 1010, 3806, 1622, 123, 1689, 5220, 2551, 3796, 4330, 5211, 5490, 114, 4398, 2841, 3062, 2078, 2138, 4263, 4082, 4197, 774, 1481, 3947, 3408, 2339, 1148, 3250, 270, 3106, 1245, 4110, 5416, 4758, 4286, 1635, 5437, 1178, 5334, 2575, 343, 3803, 3719, 2010, 3324, 625, 295, 3240, 2438, 5488, 1063, 2322, 2775, 1641, 4760, 5124, 462, 230, 5349, 1826, 2748, 4422, 98, 4351, 5177, 4305, 3846, 5197, 2131, 2905, 695, 4812, 1360, 808, 5370, 1562, 3583, 3797, 3264, 5240, 5385, 3048, 4064, 5272, 404, 4545, 82, 3466, 4658, 1661, 4221, 1774, 3463, 1100, 4449, 1726, 2791, 1102, 3714, 791, 4405, 1527, 4739, 214, 2820, 2282, 5337, 416, 4922, 2847, 1201, 2711, 1467, 1901, 5104, 5250, 4234, 1177, 5377, 2234, 767, 3499, 4280, 3922, 2281, 164, 2506, 77, 1405, 4675, 1459, 5017, 1494, 4647, 5178, 2625, 1654, 5464, 2940, 2927, 4167, 1104, 4146, 4805, 3190, 3239, 3307, 4174, 786, 1337, 2691, 1221, 749, 344, 2577, 88, 1832, 585, 1680, 4370, 2331, 26, 3794, 811, 4115, 4211, 2651, 1862, 3855, 5513, 4525, 156, 621, 2747, 4439, 1730, 3152, 4911, 5389, 3662, 2767, 703, 5123, 1127, 2986, 1729, 1544, 220, 4455, 2601, 1397, 752, 2662, 482, 5439, 2699, 1526, 3154, 3603, 3734, 4656, 5000, 3822, 3782, 2464, 2264, 643, 1711, 3523, 3269, 2842, 4615, 5500, 2635, 4949, 3412, 2024, 5224, 2602, 241, 773, 2479, 59, 3303, 2895, 1529, 318, 2458, 1083, 2254, 480, 1807, 4958, 2581, 3895, 2058, 473, 3417, 4255, 5309, 182, 5003, 2074, 3598, 126, 1616, 4242, 2100, 44, 1934, 664, 5199, 3026, 1643, 5449, 5132, 3829, 5498, 848, 3050, 676, 611, 983, 431, 906, 891, 1811, 5473, 1060, 3698, 1894, 5355, 870, 1129, 3917, 3162, 1217, 5448, 3679, 3799, 70, 2633, 5287, 2734, 616, 212, 3888, 4387, 4481, 3852, 5116, 2906, 244, 531, 691, 1485, 4981, 279, 1549, 661, 564, 4821, 3331, 3356, 384, 5546, 693, 4890, 5099, 294, 1991, 4062, 5119, 4695, 3041, 1893, 3757, 1995, 3056, 239, 4939, 433, 5216, 4015, 495, 3715, 4027, 816, 845, 1126, 5514, 526, 2460, 4441, 4701, 5265, 5367, 5256, 4400, 4317, 4288, 2529, 4459, 3569, 4794, 4759, 235, 545, 4386, 2149, 4857, 3949, 4494, 1417, 681, 3180, 4468, 4581, 4845, 3741, 3880, 1440, 1662, 106, 1821, 1328, 690, 5158, 1419, 1051, 4332, 2267, 396, 812, 2857, 4050, 4984, 4892, 2245, 1385, 4818, 3137, 5374, 1167, 5441, 3440, 537, 966, 2424, 4554, 709, 1124, 2275, 4603, 3939, 1152, 2334, 2904, 121, 5009, 2410, 3185, 1708, 3689, 498, 399, 97, 1215, 4968, 985, 1345, 2154, 1415, 413, 240, 2121, 1401, 2378, 5499, 3025, 5415, 3055, 4176, 3284, 784, 1891, 5307, 3481, 542, 4636, 2033, 2835, 4074, 3511, 4591, 3124, 208, 451, 4239, 844, 2333, 5156, 2516, 727, 2660, 2208, 3637, 1816, 1748, 915, 3551, 5, 2692, 4978, 5531, 150, 4705, 1628, 3294, 1720, 3697, 2994, 4785, 900, 2844, 710, 4630, 4835, 4276, 2997, 352, 4136, 5006, 1324, 5497, 3296, 5492, 280, 4712, 2198, 1223, 4967, 4547, 2211, 443, 337, 4988, 1413, 1190, 4749, 765, 5232, 1574, 3261, 563, 3441, 1623, 4403, 4069, 4693, 694, 4102, 143, 4421, 1091, 684, 67, 2705, 1663, 1619, 4740, 993, 31, 4889, 917, 3479, 1939, 192, 1139, 4028, 3103, 448, 3368, 43, 1634, 353, 2798, 2303, 3248, 1719, 2185, 718, 3455, 1837, 1679, 2385, 832, 3142, 518, 4836, 4147, 1084, 908, 420, 1949, 1379, 1287, 3461, 3702, 1387, 3104, 335, 5051, 4540, 2736, 662, 4953, 3798, 4118, 2771, 52, 1524, 1134, 2188, 3058, 5023, 3792, 5863, 5953, 5871, 5937, 5704, 5705, 5810, 5702, 6142, 5975, 6156, 5977, 6227, 5773, 6007, 5775, 6045, 5691, 6226, 6134, 5946, 5814, 5566, 5745, 5567, 5694, 5772, 5959, 5679, 6163, 5766, 6075, 5749, 5701, 5621, 6225, 5777, 6161, 5806, 5886, 5933, 5788, 5686, 5909, 6098, 5658, 5750, 5669, 5781, 5787, 5677, 5697, 5722, 5734, 5684, 5751, 6172, 5683, 6006, 5708, 5904, 6014, 6212, 5723, 5809, 5581, 5838, 5817, 5856, 6128, 5931, 6151, 5650, 6186, 5576, 6082, 5907, 5978, 5568, 5941, 5816, 5606, 5663, 6185, 6051, 5602, 6011, 5682, 5972, 5971, 5610, 5914, 5692, 5738, 5580, 5637, 6182, 5672, 5556, 5908, 6213, 5967, 5695, 6048, 5604, 5797, 5559, 6217, 5560, 5635, 5862, 5922, 6027, 6123, 5619, 6211, 5918, 6004, 6150, 5990, 6093, 5665, 5562, 5653, 5711, 6136, 5585, 6206, 5795, 5548, 5936, 6086, 5631, 5553, 5872, 5689, 5993, 5618, 5740, 5874, 5976, 5826, 5831, 5703, 6043, 5601, 5866, 5849, 5744, 5961, 5586, 6068, 5969, 5955, 5710, 5644, 6198, 5611, 5577, 5876, 5613, 5754, 6054, 5698, 5639, 6076, 5726, 5584, 6113, 5905, 5930, 5865, 5842, 5759, 6057, 5966, 5868, 6021, 6037, 6232, 6095, 5819, 5847, 5807, 5989, 5564, 5674, 5599, 6036, 5783, 5589, 5996, 6032, 5640, 5928, 5627, 6114, 6055, 5864, 6122, 6012, 5900, 5608, 6008, 6188, 6069, 6230, 5720, 5700, 6026, 5598, 6116, 5656, 5671, 6221, 6233, 6035, 5714, 5792, 6016, 5859, 5894, 6205, 5592, 6139, 6081, 5615, 5570, 6154, 5769, 6064, 6235, 5885, 6052, 6234, 5687, 5696, 6084, 6060, 6042, 5668, 6180, 6061, 5869, 5903, 5853, 5727, 5693, 5641, 6050, 5837, 5762, 6034, 5730, 5774, 5678, 6120, 5685, 6141, 5587, 5916, 5633, 6105, 5673, 5549, 5911, 5575, 6170, 6107, 6077, 6124, 5739, 5956, 6017, 5763, 5699, 6009, 6079, 5997, 5906, 5893, 5771, 5877, 5881, 5579, 6097, 6103, 5661, 5889, 5821, 5652, 6010, 5813, 5785, 5753, 5985, 5741, 6039, 5634, 6162, 6088, 6000, 6002, 5752, 5796, 6130, 5880, 5596, 5603, 6115, 5551, 5824, 5715, 5898, 5632, 5962, 6210, 5948, 6143, 5664, 6193, 6229, 6145, 5742, 5902, 5844, 5605, 6168, 5801, 5622, 6158, 6040, 6153, 5636, 6214, 6063, 6222, 5811, 6005, 5554, 5793, 5873, 5612, 5957, 6159, 6067, 6133, 6176, 5768, 6238, 5550, 5707, 5573, 6071, 5724, 5761, 5732, 5991, 5625, 6165, 5895, 5899, 5848, 6031, 5552, 6106, 5944, 6102, 5646, 5561, 6096, 5735, 6166, 5680, 5780, 5624, 6171, 5770, 6181, 5861, 5983, 5992, 6138, 6119, 6126, 5890, 6117, 5607, 5841, 5995, 6074, 5609, 5597, 6100, 5799, 6203, 6195, 5892, 5758, 5659, 5764, 5979, 6207, 5851, 5857, 6192, 5980, 5920, 5595, 5578, 5834, 6132, 5725, 6184, 5940, 6177, 5947, 5891, 6216, 6023, 5943, 6228, 6003, 5654, 6137, 6201, 6131, 5791, 6041, 5998, 5828, 5840, 5845, 6190, 6148, 6018, 6085, 5712, 5860, 5855, 6020, 5670, 5932, 6108, 5942, 6066, 6092, 5676, 5569, 5645, 5590, 6033, 5913, 5884, 5921, 5642, 5827, 5958, 5760, 5973, 6078, 6218, 6024, 6224, 6111, 5623, 5822, 6013, 6155, 6196, 6169, 5835, 5790, 5901, 5666, 5968, 5736, 5591, 5832, 5765, 5852, 6070, 5638, 5729, 6147, 5630, 5981, 6110, 6197, 5647, 6146, 5870, 5818, 5755, 6140, 5776, 5929, 5925, 5651, 6028, 6049, 6090, 5887, 5949, 6164, 5939, 5829, 6089, 5616, 5748, 5923, 6209, 5878, 6175, 6109, 5600, 5583, 5915, 5988, 5910, 6101, 5743, 5563, 5917, 5614, 6194, 5803, 6094, 6179, 5690, 6183, 6220, 6038, 6065, 6208, 5867, 6044, 5617, 5655, 5574, 5688, 5657, 5897, 6127, 6144, 5728, 5982, 5970, 5731, 6030, 6200, 5588, 5974, 6112, 6118, 5802, 5713, 5950, 6053, 6202, 5850, 5782, 5805, 5789, 6022, 5994, 5778, 5786, 5716, 5825, 5746, 5804, 5718, 5709, 6025, 5800, 5882, 6189, 5935, 5558, 5675, 5830, 5626, 6001, 5815, 5648, 5945, 5833, 5986, 5812, 5756, 6091, 5854, 5875, 5808, 5719, 5784, 5965, 6047, 6149, 5572, 5721, 5919, 6104, 5858, 6135, 6083, 6191, 5896, 6019, 5628, 6236, 6080, 6239, 5987, 5798, 6167, 6160, 6099, 6215, 5951, 5747, 5963, 5912, 5593, 6056, 5934, 6157, 5926, 5565, 5954, 5571, 6152, 5927, 5733, 6062, 6240, 6129, 5938, 5779, 5924, 5594, 5681, 5879, 5794, 6087, 6219, 5843, 6125, 5820, 6204, 6173, 5649, 6231, 5952, 6073, 5999, 5660, 5836, 6015, 6178, 5667, 5888, 5620, 5717, 5643, 5557, 5582, 5555, 6121, 5706, 6174, 6046, 6058, 5767, 5960, 5757, 5846, 6029, 6237, 5839, 5823, 6199, 5964, 5662, 5737, 6223, 6059, 6187, 6072, 5629, 5883, 5984]\n",
            "6241\n",
            "[[0 1 0 0 0 0]\n",
            " [1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0]\n",
            " [1 0 0 0 0 0]]\n",
            "[[0 0 0 1 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 0 0 0 0 1]\n",
            " [1 0 0 0 0 0]\n",
            " [0 0 1 0 0 0]]\n",
            "(4994, 300) (4994, 6) (693, 300) (693, 6) (10424, 300) (10424, 6)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "# import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from math import log\n",
        "from sklearn import svm\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "\n",
        "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n",
        "# build corpus\n",
        "dataset = dataet_name\n",
        "\n",
        "\n",
        "# Read Word Vectors\n",
        "# word_vector_file = 'data/glove.6B/glove.6B.300d.txt'\n",
        "# word_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n",
        "#_, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
        "# word_embeddings_dim = len(embd[0])\n",
        "\n",
        "word_embeddings_dim = 300\n",
        "word_vector_map = {}\n",
        "\n",
        "# shulffing\n",
        "doc_name_list = []\n",
        "doc_train_list = []\n",
        "doc_test_list = []\n",
        "\n",
        "with open('/content/data/' + dataset + '.txt', 'r',errors='replace') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        doc_name_list.append(line.strip())\n",
        "        temp = line.split(\"\\t\")\n",
        "        if temp[1].find('test') != -1:\n",
        "            doc_test_list.append(line.strip())\n",
        "        elif temp[1].find('train') != -1:\n",
        "            doc_train_list.append(line.strip())\n",
        "# print(doc_train_list)\n",
        "# print(doc_test_list)\n",
        "\n",
        "doc_content_list = []\n",
        "with open('/content/data/corpus/' + dataset + '.txt','r',errors='replace') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        doc_content_list.append(line.strip())\n",
        "# print(doc_content_list)\n",
        "\n",
        "train_ids = []\n",
        "for train_name in doc_train_list:\n",
        "    train_id = doc_name_list.index(train_name)\n",
        "    train_ids.append(train_id)\n",
        "# print(train_ids)\n",
        "random.shuffle(train_ids)\n",
        "\n",
        "# partial labeled data\n",
        "#train_ids = train_ids[:int(0.2 * len(train_ids))]\n",
        "\n",
        "train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
        "with open('/content/data/' + dataset + '.train.index', 'w') as f:\n",
        "    f.write(train_ids_str)\n",
        "\n",
        "\n",
        "test_ids = []\n",
        "for test_name in doc_test_list:\n",
        "    test_id = doc_name_list.index(test_name)\n",
        "    test_ids.append(test_id)\n",
        "print(test_ids)\n",
        "random.shuffle(test_ids)\n",
        "\n",
        "test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
        "with open('/content/data/' + dataset + '.test.index', 'w') as f:\n",
        "    f.write(test_ids_str)\n",
        "\n",
        "\n",
        "ids = train_ids + test_ids\n",
        "print(ids)\n",
        "print(len(ids))\n",
        "\n",
        "shuffle_doc_name_list = []\n",
        "shuffle_doc_words_list = []\n",
        "for id in ids:\n",
        "    # print(id)\n",
        "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
        "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
        "shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
        "shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
        "\n",
        "with open('/content/data/' + dataset + '_shuffle.txt', 'w') as f:\n",
        "    f.write(shuffle_doc_name_str)\n",
        "\n",
        "with open('/content/data/corpus/' + dataset + '_shuffle.txt', 'w') as f:\n",
        "    f.write(shuffle_doc_words_str)\n",
        "\n",
        "\n",
        "# build vocab\n",
        "word_freq = {}\n",
        "word_set = set()\n",
        "for doc_words in shuffle_doc_words_list:\n",
        "    words = doc_words.split()\n",
        "    for word in words:\n",
        "        word_set.add(word)\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "\n",
        "vocab = list(word_set)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_doc_list = {}\n",
        "\n",
        "for i in range(len(shuffle_doc_words_list)):\n",
        "    doc_words = shuffle_doc_words_list[i]\n",
        "    words = doc_words.split()\n",
        "    appeared = set()\n",
        "    for word in words:\n",
        "        if word in appeared:\n",
        "            continue\n",
        "        if word in word_doc_list:\n",
        "            doc_list = word_doc_list[word]\n",
        "            doc_list.append(i)\n",
        "            word_doc_list[word] = doc_list\n",
        "        else:\n",
        "            word_doc_list[word] = [i]\n",
        "        appeared.add(word)\n",
        "\n",
        "word_doc_freq = {}\n",
        "for word, doc_list in word_doc_list.items():\n",
        "    word_doc_freq[word] = len(doc_list)\n",
        "\n",
        "word_id_map = {}\n",
        "for i in range(vocab_size):\n",
        "    word_id_map[vocab[i]] = i\n",
        "\n",
        "vocab_str = '\\n'.join(vocab)\n",
        "\n",
        "with open('/content/data/corpus/' + dataset + '_vocab.txt', 'w') as f:\n",
        "    f.write(vocab_str)\n",
        "\n",
        "label_set = set()\n",
        "for doc_meta in shuffle_doc_name_list:\n",
        "    temp = doc_meta.split('\\t')\n",
        "    label_set.add(temp[2])\n",
        "label_list = list(label_set)\n",
        "\n",
        "label_list_str = '\\n'.join(label_list)\n",
        "with open('/content/data/corpus/' + dataset + '_labels.txt', 'w') as f:\n",
        "    f.write(label_list_str)\n",
        "\n",
        "\n",
        "# x: feature vectors of training docs, no initial features\n",
        "# slect 90% training set\n",
        "train_size = len(train_ids)\n",
        "val_size = int(0.1 * train_size)\n",
        "real_train_size = train_size - val_size  # - int(0.5 * train_size)\n",
        "# different training rates\n",
        "\n",
        "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
        "real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
        "\n",
        "with open('/content/data/' + dataset + '.real_train.name', 'w') as f:\n",
        "    f.write(real_train_doc_names_str)\n",
        "\n",
        "\n",
        "row_x = []\n",
        "col_x = []\n",
        "data_x = []\n",
        "for i in range(real_train_size):\n",
        "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
        "    doc_words = shuffle_doc_words_list[i]\n",
        "    words = doc_words.split()\n",
        "    doc_len = len(words)\n",
        "    for word in words:\n",
        "        if word in word_vector_map:\n",
        "            word_vector = word_vector_map[word]\n",
        "            # print(doc_vec)\n",
        "            # print(np.array(word_vector))\n",
        "            doc_vec = doc_vec + np.array(word_vector)\n",
        "\n",
        "    for j in range(word_embeddings_dim):\n",
        "        row_x.append(i)\n",
        "        col_x.append(j)\n",
        "        # np.random.uniform(-0.25, 0.25)\n",
        "        data_x.append(doc_vec[j] / doc_len)  # doc_vec[j]/ doc_len\n",
        "\n",
        "# x = sp.csr_matrix((real_train_size, word_embeddings_dim), dtype=np.float32)\n",
        "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n",
        "    real_train_size, word_embeddings_dim))\n",
        "\n",
        "y = []\n",
        "for i in range(real_train_size):\n",
        "    doc_meta = shuffle_doc_name_list[i]\n",
        "    temp = doc_meta.split('\\t')\n",
        "    label = temp[2]\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    label_index = label_list.index(label)\n",
        "    one_hot[label_index] = 1\n",
        "    y.append(one_hot)\n",
        "y = np.array(y)\n",
        "print(y)\n",
        "\n",
        "# tx: feature vectors of test docs, no initial features\n",
        "test_size = len(test_ids)\n",
        "\n",
        "row_tx = []\n",
        "col_tx = []\n",
        "data_tx = []\n",
        "for i in range(test_size):\n",
        "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
        "    doc_words = shuffle_doc_words_list[i + train_size]\n",
        "    words = doc_words.split()\n",
        "    doc_len = len(words)\n",
        "    for word in words:\n",
        "        if word in word_vector_map:\n",
        "            word_vector = word_vector_map[word]\n",
        "            doc_vec = doc_vec + np.array(word_vector)\n",
        "\n",
        "    for j in range(word_embeddings_dim):\n",
        "        row_tx.append(i)\n",
        "        col_tx.append(j)\n",
        "        # np.random.uniform(-0.25, 0.25)\n",
        "        data_tx.append(doc_vec[j] / doc_len)  # doc_vec[j] / doc_len\n",
        "\n",
        "# tx = sp.csr_matrix((test_size, word_embeddings_dim), dtype=np.float32)\n",
        "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
        "                   shape=(test_size, word_embeddings_dim))\n",
        "\n",
        "ty = []\n",
        "for i in range(test_size):\n",
        "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
        "    temp = doc_meta.split('\\t')\n",
        "    label = temp[2]\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    label_index = label_list.index(label)\n",
        "    one_hot[label_index] = 1\n",
        "    ty.append(one_hot)\n",
        "ty = np.array(ty)\n",
        "print(ty)\n",
        "\n",
        "# allx: the the feature vectors of both labeled and unlabeled training instances\n",
        "# (a superset of x)\n",
        "# unlabeled training instances -> words\n",
        "\n",
        "word_vectors = np.random.uniform(-0.01, 0.01,\n",
        "                                 (vocab_size, word_embeddings_dim))\n",
        "\n",
        "for i in range(len(vocab)):\n",
        "    word = vocab[i]\n",
        "    if word in word_vector_map:\n",
        "        vector = word_vector_map[word]\n",
        "        word_vectors[i] = vector\n",
        "\n",
        "row_allx = []\n",
        "col_allx = []\n",
        "data_allx = []\n",
        "\n",
        "for i in range(train_size):\n",
        "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
        "    doc_words = shuffle_doc_words_list[i]\n",
        "    words = doc_words.split()\n",
        "    doc_len = len(words)\n",
        "    for word in words:\n",
        "        if word in word_vector_map:\n",
        "            word_vector = word_vector_map[word]\n",
        "            doc_vec = doc_vec + np.array(word_vector)\n",
        "\n",
        "    for j in range(word_embeddings_dim):\n",
        "        row_allx.append(int(i))\n",
        "        col_allx.append(j)\n",
        "        # np.random.uniform(-0.25, 0.25)\n",
        "        data_allx.append(doc_vec[j] / doc_len)  # doc_vec[j]/doc_len\n",
        "for i in range(vocab_size):\n",
        "    for j in range(word_embeddings_dim):\n",
        "        row_allx.append(int(i + train_size))\n",
        "        col_allx.append(j)\n",
        "        data_allx.append(word_vectors.item((i, j)))\n",
        "\n",
        "\n",
        "row_allx = np.array(row_allx)\n",
        "col_allx = np.array(col_allx)\n",
        "data_allx = np.array(data_allx)\n",
        "\n",
        "allx = sp.csr_matrix(\n",
        "    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n",
        "\n",
        "ally = []\n",
        "for i in range(train_size):\n",
        "    doc_meta = shuffle_doc_name_list[i]\n",
        "    temp = doc_meta.split('\\t')\n",
        "    label = temp[2]\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    label_index = label_list.index(label)\n",
        "    one_hot[label_index] = 1\n",
        "    ally.append(one_hot)\n",
        "\n",
        "for i in range(vocab_size):\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    ally.append(one_hot)\n",
        "\n",
        "ally = np.array(ally)\n",
        "\n",
        "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
        "\n",
        "'''\n",
        "Doc word heterogeneous graph\n",
        "'''\n",
        "\n",
        "# word co-occurence with context windows\n",
        "window_size = 20\n",
        "windows = []\n",
        "\n",
        "for doc_words in shuffle_doc_words_list:\n",
        "    words = doc_words.split()\n",
        "    length = len(words)\n",
        "    if length <= window_size:\n",
        "        windows.append(words)\n",
        "    else:\n",
        "        # print(length, length - window_size + 1)\n",
        "        for j in range(length - window_size + 1):\n",
        "            window = words[j: j + window_size]\n",
        "            windows.append(window)\n",
        "            # print(window)\n",
        "\n",
        "\n",
        "word_window_freq = {}\n",
        "for window in windows:\n",
        "    appeared = set()\n",
        "    for i in range(len(window)):\n",
        "        if window[i] in appeared:\n",
        "            continue\n",
        "        if window[i] in word_window_freq:\n",
        "            word_window_freq[window[i]] += 1\n",
        "        else:\n",
        "            word_window_freq[window[i]] = 1\n",
        "        appeared.add(window[i])\n",
        "\n",
        "word_pair_count = {}\n",
        "for window in windows:\n",
        "    for i in range(1, len(window)):\n",
        "        for j in range(0, i):\n",
        "            word_i = window[i]\n",
        "            word_i_id = word_id_map[word_i]\n",
        "            word_j = window[j]\n",
        "            word_j_id = word_id_map[word_j]\n",
        "            if word_i_id == word_j_id:\n",
        "                continue\n",
        "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
        "            if word_pair_str in word_pair_count:\n",
        "                word_pair_count[word_pair_str] += 1\n",
        "            else:\n",
        "                word_pair_count[word_pair_str] = 1\n",
        "            # two orders\n",
        "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
        "            if word_pair_str in word_pair_count:\n",
        "                word_pair_count[word_pair_str] += 1\n",
        "            else:\n",
        "                word_pair_count[word_pair_str] = 1\n",
        "\n",
        "row = []\n",
        "col = []\n",
        "weight = []\n",
        "\n",
        "# pmi as weights\n",
        "\n",
        "num_window = len(windows)\n",
        "\n",
        "for key in word_pair_count:\n",
        "    temp = key.split(',')\n",
        "    i = int(temp[0])\n",
        "    j = int(temp[1])\n",
        "    count = word_pair_count[key]\n",
        "    word_freq_i = word_window_freq[vocab[i]]\n",
        "    word_freq_j = word_window_freq[vocab[j]]\n",
        "    pmi = log((1.0 * count / num_window) /\n",
        "              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
        "    if pmi <= 0:\n",
        "        continue\n",
        "    row.append(train_size + i)\n",
        "    col.append(train_size + j)\n",
        "    weight.append(pmi)\n",
        "\n",
        "# word vector cosine similarity as weights\n",
        "\n",
        "'''\n",
        "for i in range(vocab_size):\n",
        "    for j in range(vocab_size):\n",
        "        if vocab[i] in word_vector_map and vocab[j] in word_vector_map:\n",
        "            vector_i = np.array(word_vector_map[vocab[i]])\n",
        "            vector_j = np.array(word_vector_map[vocab[j]])\n",
        "            similarity = 1.0 - cosine(vector_i, vector_j)\n",
        "            if similarity > 0.9:\n",
        "                print(vocab[i], vocab[j], similarity)\n",
        "                row.append(train_size + i)\n",
        "                col.append(train_size + j)\n",
        "                weight.append(similarity)\n",
        "'''\n",
        "# doc word frequency\n",
        "doc_word_freq = {}\n",
        "\n",
        "for doc_id in range(len(shuffle_doc_words_list)):\n",
        "    doc_words = shuffle_doc_words_list[doc_id]\n",
        "    words = doc_words.split()\n",
        "    for word in words:\n",
        "        word_id = word_id_map[word]\n",
        "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
        "        if doc_word_str in doc_word_freq:\n",
        "            doc_word_freq[doc_word_str] += 1\n",
        "        else:\n",
        "            doc_word_freq[doc_word_str] = 1\n",
        "\n",
        "for i in range(len(shuffle_doc_words_list)):\n",
        "    doc_words = shuffle_doc_words_list[i]\n",
        "    words = doc_words.split()\n",
        "    doc_word_set = set()\n",
        "    for word in words:\n",
        "        if word in doc_word_set:\n",
        "            continue\n",
        "        j = word_id_map[word]\n",
        "        key = str(i) + ',' + str(j)\n",
        "        freq = doc_word_freq[key]\n",
        "        if i < train_size:\n",
        "            row.append(i)\n",
        "        else:\n",
        "            row.append(i + vocab_size)\n",
        "        col.append(train_size + j)\n",
        "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
        "                  word_doc_freq[vocab[j]])\n",
        "        weight.append(freq * idf)\n",
        "        doc_word_set.add(word)\n",
        "\n",
        "node_size = train_size + vocab_size + test_size\n",
        "adj = sp.csr_matrix(\n",
        "    (weight, (row, col)), shape=(node_size, node_size))\n",
        "\n",
        "# dump objects\n",
        "with open(\"/content/data/ind.{}.x\".format(dataset), 'wb') as f:\n",
        "    pkl.dump(x, f)\n",
        "\n",
        "with open(\"/content/data/ind.{}.y\".format(dataset), 'wb') as f:\n",
        "    pkl.dump(y, f)\n",
        "\n",
        "with open(\"/content/data/ind.{}.tx\".format(dataset), 'wb') as f:\n",
        "    pkl.dump(tx, f)\n",
        "\n",
        "with open(\"/content/data/ind.{}.ty\".format(dataset), 'wb') as f:\n",
        "    pkl.dump(ty, f)\n",
        "\n",
        "with open(\"/content/data/ind.{}.allx\".format(dataset), 'wb') as f:\n",
        "    pkl.dump(allx, f)\n",
        "\n",
        "with open(\"/content/data/ind.{}.ally\".format(dataset), 'wb') as f:\n",
        "    pkl.dump(ally, f)\n",
        "\n",
        "with open(\"/content/data/ind.{}.adj\".format(dataset), 'wb') as f:\n",
        "    pkl.dump(adj, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kI5wBoAQVUqy"
      },
      "outputs": [],
      "source": [
        "word_vector_map = {}\n",
        "\n",
        "# shulffing\n",
        "doc_name_list = []\n",
        "doc_train_list = []\n",
        "doc_test_list = []\n",
        "\n",
        "with open('/content/data/' + dataset + '.txt', 'r',errors='replace') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        doc_name_list.append(line.strip())\n",
        "        temp = line.split(\"\\t\")\n",
        "        if temp[1].find('test') != -1:\n",
        "            doc_test_list.append(line.strip())\n",
        "        elif temp[1].find('train') != -1:\n",
        "            doc_train_list.append(line.strip())\n",
        "# print(doc_train_list)\n",
        "# print(doc_test_list)\n",
        "\n",
        "doc_content_list = []\n",
        "with open('/content/data/corpus/' + dataset + '.txt','r',errors='replace') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        doc_content_list.append(line.strip())\n",
        "# print(doc_content_list)\n",
        "\n",
        "train_ids = []\n",
        "for train_name in doc_train_list:\n",
        "    train_id = doc_name_list.index(train_name)\n",
        "    train_ids.append(train_id)\n",
        "# print(train_ids)\n",
        "# random.shuffle(train_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuY6Va1TmOTd"
      },
      "source": [
        "### delete to freeram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lMQrMwqXmQYk"
      },
      "outputs": [],
      "source": [
        "# del(doc_name_list)\n",
        "# del(doc_train_list)\n",
        "# del(doc_test_list)\n",
        "# del(lines)\n",
        "# del(train_ids)\n",
        "# del(train_ids_str)\n",
        "# del(test_ids)\n",
        "# del(test_ids_str)\n",
        "# del(ids)\n",
        "# del(shuffle_doc_name_list)\n",
        "# del(shuffle_doc_words_list)\n",
        "# del(word_doc_list)\n",
        "# del(word_id_map)\n",
        "# del(vocab_str)\n",
        "# del(label_set)\n",
        "# del(train_size)\n",
        "# del(real_train_doc_names)\n",
        "# del(real_train_doc_names_str)\n",
        "# del(row_x)\n",
        "# del(col_x)\n",
        "# del(data_x)\n",
        "# del(x)\n",
        "# del(y)\n",
        "# del(row_tx)\n",
        "# del(col_tx)\n",
        "# del(data_tx)\n",
        "# del(tx)\n",
        "# del(ty)\n",
        "# del(word_vectors)\n",
        "# del(row_allx)\n",
        "# del(col_allx)\n",
        "# del(data_allx)\n",
        "# del(row_allx)\n",
        "# del(col_allx)\n",
        "# del(data_allx)\n",
        "# del(allx)\n",
        "# del(ally)\n",
        "# del(windows)\n",
        "# del(word_window_freq)\n",
        "# del(word_pair_count)\n",
        "# del(row)\n",
        "# del(col)\n",
        "# del(weight)\n",
        "# del(num_window)\n",
        "# del(doc_word_freq)\n",
        "# del()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAmJ-_ndwFag"
      },
      "source": [
        "## training data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "s93YEqi8vLSZ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "# from utils import load_corpus,normalize_adj\n",
        "from scipy.sparse import eye\n",
        "import torch\n",
        "import dgl\n",
        "def encode_input(text, tokenizer):\n",
        "    input = tokenizer(text, max_length=256, truncation=True, padding='max_length', return_tensors='pt')\n",
        "#     print(input.keys())\n",
        "    return input.input_ids, input.attention_mask\n",
        "class Data_set(Dataset):\n",
        "    def __init__(self,name):\n",
        "        adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(name)\n",
        "        doc_mask=train_mask+test_mask+val_mask\n",
        "        adj=normalize_adj(adj+eye(adj.shape[0]))\n",
        "        train_num=train_mask.sum().item()\n",
        "        val_num=val_mask.sum().item()\n",
        "        test_num=test_mask.sum().item()\n",
        "        node_size=adj.shape[0]\n",
        "        y=torch.tensor(y_train+y_val+y_test)\n",
        "        self.y=torch.argmax(y,-1)\n",
        "\n",
        "        self.train_index=[i for i in range(train_num+val_num)] + [i for i in range(node_size-test_num,node_size)]\n",
        "        corpse_file = open('/content/data/corpus/' + name +'_shuffle.txt').readlines()\n",
        "        token=AutoTokenizer.from_pretrained('vinai/phobert-base')\n",
        "        # token=AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "        \n",
        "        self.dataset,self.attention_mask=encode_input(corpse_file,token)\n",
        "        self.attention_mask=torch.tensor(self.attention_mask)\n",
        "        self.dataset=torch.tensor(self.dataset)\n",
        "        self.graph=dgl.from_scipy(adj,eweight_name='w')\n",
        "        self.graph.ndata['label']=self.y\n",
        "        self.label_num=len(y_train[0])\n",
        "        self.graph.edata['w']=self.graph.edata['w'].float()\n",
        "        self.graph.ndata['train_mask']=torch.tensor(train_mask)\n",
        "        self.graph.ndata['valid_mask']=torch.tensor(val_mask)\n",
        "        self.graph.ndata['test_mask']=torch.tensor(test_mask)\n",
        "        self.train_mask=train_mask\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    def __getitem__(self, item):\n",
        "        return self.dataset[item],self.y[self.train_index[item]],self.attention_mask[item],self.train_mask[self.train_index[item]],self.train_index[item]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlbcFsY4y7Vb"
      },
      "source": [
        "## pretrain(fine tune) data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DOKNKP_TzBid"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch import tensor\n",
        "import pickle\n",
        "\n",
        "\n",
        "class DataSet(Dataset):\n",
        "    def __init__(self, name, usage, label_dict=None):\n",
        "        tokenizers = AutoTokenizer.from_pretrained('vinai/phobert-base')\n",
        "        # tokenizers = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "        \n",
        "        # data=open(usage+'_data.txt').readlines()\n",
        "        # label=open(usage+'_label.txt').readlines()\n",
        "        # self.data=[tokenizers.encode(each,max_length=512) for each in data]\n",
        "        # self.label=[int(i) for i in label]\n",
        "        # self.class_num=23\n",
        "        label_path = '/content/data/' + name + '_labels.pkl'\n",
        "        indexs = '/content/data/' + name + '_indexs.pkl'\n",
        "        current_usage = '/content/data/' + name + '_' + usage + '_index.pkl'\n",
        "        current_usage = pickle.load(open(current_usage, 'rb'))\n",
        "        orig_data_path = '/content/data/corpus/' + name + '.txt'\n",
        "        orig_data = open(orig_data_path).readlines()\n",
        "        indexs = pickle.load(open(indexs, 'rb'))\n",
        "        labels = pickle.load(open(label_path, 'rb'))\n",
        "        orig_data = [orig_data[i] for i in indexs]\n",
        "        orig_data = [tensor(tokenizers.encode(each, max_length=256,truncation=True)) for each in orig_data]\n",
        "        self.data = [orig_data[i] for i in current_usage]\n",
        "        self.label = [labels[i] for i in current_usage]\n",
        "        self.class_num = len(pickle.load(open('/content/data/' + name + '_label_dict.pkl', 'rb')))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return tensor(self.data[item]), tensor(self.label[item])\n",
        "# data=DataSet('ohsumed','train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IepFinMmzJXb"
      },
      "source": [
        "## remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjw-nCvYzK21",
        "outputId": "53b60c0f-a4c9-40ef-de3e-d0f015e32f61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'their', 'couldn', 'shouldn', 'theirs', 'more', 'having', 'during', 'shan', 's', 'the', 'have', 'who', 'whom', 'wasn', 'yourselves', 'those', 'most', 'own', \"it's\", 'because', 'weren', 'myself', 'he', 'once', 'again', 'me', 'where', 'all', \"shan't\", 'further', 'she', 'that', 'now', 'o', 'didn', 'we', \"couldn't\", \"won't\", 'my', 'did', 'this', 'but', 'if', 'our', 've', \"should've\", 'at', 'off', 'are', 'these', 're', 'y', 'which', 'than', \"don't\", 'too', 'll', 'a', 'yours', 'yourself', \"haven't\", \"hadn't\", \"you're\", 'what', 'under', 'haven', 'don', \"she's\", 'or', 'was', 'her', 'ma', 'needn', 'just', 'be', 'below', \"shouldn't\", 'same', 'them', 'mightn', 'm', 'both', 'hasn', 'ourselves', 'herself', 'been', 't', 'ours', 'over', 'for', 'has', 'as', \"didn't\", 'about', 'its', 'himself', \"weren't\", 'such', 'wouldn', 'before', \"you'd\", 'am', 'you', 'hadn', \"mustn't\", 'themselves', 'in', 'his', 'and', 'between', 'few', \"doesn't\", 'here', 'aren', 'had', 'how', 'will', 'do', 'not', 'with', 'is', 'to', 'when', 'isn', 'does', 'him', 'above', 'doesn', 'each', 'up', \"you'll\", 'your', 'hers', 'won', 'nor', 'until', 'there', 'an', 'after', 'doing', \"wasn't\", \"wouldn't\", 'so', 'on', 'by', \"needn't\", 'ain', \"aren't\", 'mustn', \"hasn't\", 'into', 'they', 'no', 'very', 'should', 'of', 'can', \"mightn't\", 'some', \"that'll\", 'down', 'from', \"you've\", 'why', 'any', 'while', 'itself', 'being', 'through', 'against', 'only', 'it', 'then', 'were', 'other', \"isn't\", 'd', 'out', 'i'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6241/6241 [00:00<00:00, 1284176.38it/s]\n",
            "100%|██████████| 6241/6241 [00:00<00:00, 169170.85it/s]\n",
            "100%|██████████| 6241/6241 [00:00<00:00, 129473.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min_len : 0\n",
            "Max_len : 155\n",
            "Average_len : 13.832719115526357\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import wordnet as wn\n",
        "# from utils import clean_str\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "dataset = dataet_name\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "\n",
        "# Read Word Vectors\n",
        "# word_vector_file = 'data/glove.6B/glove.6B.200d.txt'\n",
        "# vocab, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
        "# word_embeddings_dim = len(embd[0])\n",
        "# dataset = '20ng'\n",
        "\n",
        "doc_content_list = []\n",
        "# with open('data/wiki_long_abstracts_en_text.txt', 'r') as f:\n",
        "with open('/content/data/corpus/' + dataset + '.txt', 'rb') as f:\n",
        "    for line in tqdm(f.readlines()):\n",
        "        doc_content_list.append(line.strip().decode('latin1'))\n",
        "\n",
        "word_freq = {}  # to remove rare words\n",
        "\n",
        "for doc_content in tqdm(doc_content_list):\n",
        "    temp = clean_str(doc_content)\n",
        "    words = temp.split()\n",
        "    for word in words:\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "\n",
        "clean_docs = []\n",
        "for doc_content in tqdm(doc_content_list):\n",
        "    temp = clean_str(doc_content)\n",
        "    words = temp.split()\n",
        "    doc_words = []\n",
        "    for word in words:\n",
        "        # word not in stop_words and word_freq[word] >= 5\n",
        "        if dataset == 'mr':\n",
        "            doc_words.append(word)\n",
        "        elif word not in stop_words and word_freq[word] >= 5:\n",
        "            doc_words.append(word)\n",
        "\n",
        "    doc_str = ' '.join(doc_words).strip()\n",
        "    # if doc_str == '':\n",
        "    # doc_str = temp\n",
        "    clean_docs.append(doc_str)\n",
        "\n",
        "clean_corpus_str = '\\n'.join(clean_docs)\n",
        "\n",
        "with open('/content/data/corpus/' + dataset + '.clean.txt', 'w') as f:\n",
        "    f.write(clean_corpus_str)\n",
        "\n",
        "# dataset = '20ng'\n",
        "min_len = 10000\n",
        "aver_len = 0\n",
        "max_len = 0\n",
        "\n",
        "with open('/content/data/corpus/' + dataset + '.clean.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        temp = line.split()\n",
        "        aver_len = aver_len + len(temp)\n",
        "        if len(temp) < min_len:\n",
        "            min_len = len(temp)\n",
        "        if len(temp) > max_len:\n",
        "            max_len = len(temp)\n",
        "\n",
        "aver_len = 1.0 * aver_len / len(lines)\n",
        "print('Min_len : ' + str(min_len))\n",
        "print('Max_len : ' + str(max_len))\n",
        "print('Average_len : ' + str(aver_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9x8LC2ow58s",
        "outputId": "c1f12c6a-c8f3-4ed0-c4b7-e5cd0fee2a7d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6241/6241 [00:00<00:00, 891029.04it/s]\n",
            "100%|██████████| 6241/6241 [00:00<00:00, 127846.89it/s]\n"
          ]
        }
      ],
      "source": [
        "doc_content_list = []\n",
        "dataset = 'VSMEC'\n",
        "# with open('data/wiki_long_abstracts_en_text.txt', 'r') as f:\n",
        "with open('/content/data/corpus/' + dataset + '.txt', 'rb') as f:\n",
        "    for line in tqdm(f.readlines()):\n",
        "        doc_content_list.append(line.strip().decode('utf-8'))\n",
        "\n",
        "word_freq = {}  # to remove rare words\n",
        "c = []\n",
        "for doc_content in tqdm(doc_content_list):\n",
        "    temp = clean_str(doc_content)\n",
        "    words = temp.split()\n",
        "    c.append(words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJam6vuZz3rr"
      },
      "source": [
        "## tokenize sentence "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMv8NlB_zLGH",
        "outputId": "f9b3bf07-4b79-4c43-fd5e-1f2ccebd197b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6241/6241 [00:00<00:00, 551168.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generating word relations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 17012/17012 [00:04<00:00, 3959.60it/s]\n"
          ]
        }
      ],
      "source": [
        "def convert_data_to_index(context):\n",
        "    dictionary = {}\n",
        "    index_context = []\n",
        "    context_length = len(context)\n",
        "    for line in context:\n",
        "        line = line.strip().lower().split(' ')\n",
        "        for word in line:\n",
        "            if word not in dictionary.keys():\n",
        "                dictionary[word] = len(dictionary.keys()) + context_length\n",
        "        index_context.append([dictionary[each] for each in line])\n",
        "    return dictionary, index_context\n",
        "\n",
        "\n",
        "def word_window_num(windows):\n",
        "    single_word_fluency = collections.defaultdict(int)\n",
        "    tuple_word_fluency = collections.defaultdict(int)\n",
        "    for window in tqdm(windows):\n",
        "        current_appear = set()\n",
        "        for i in range(len(window)):\n",
        "            if window[i] not in current_appear:\n",
        "                single_word_fluency[window[i]] += 1\n",
        "                current_appear.add(window[i])\n",
        "            for j in range(i + 1, len(window)):\n",
        "                str1=str(window[i])+','+str(window[j])\n",
        "                str2=str(window[j])+','+str(window[i])\n",
        "                if window[i] == window[j]:\n",
        "                    continue\n",
        "                if str1 in current_appear or str2 in current_appear:\n",
        "                    continue\n",
        "                else:\n",
        "                    tuple_word_fluency[str1] += 1\n",
        "                    tuple_word_fluency[str2] += 1\n",
        "                    current_appear.add(str1)\n",
        "                    current_appear.add(str2)\n",
        "    return single_word_fluency, tuple_word_fluency\n",
        "\n",
        "\n",
        "def word_document(indexed_document, start, end, weight):\n",
        "    word_document = collections.defaultdict(int)\n",
        "    for document in indexed_document:\n",
        "        appear = set()\n",
        "        for word in document:\n",
        "            if word not in appear:\n",
        "                word_document[word] += 1\n",
        "                appear.add(word)\n",
        "    for i in range(len(indexed_document)):\n",
        "        word_dict = collections.defaultdict(int)\n",
        "        line = indexed_document[i]\n",
        "        for word in line:\n",
        "            word_dict[word] += 1\n",
        "        for key in word_dict.keys():\n",
        "            start.append(i)\n",
        "            end.append(key)\n",
        "            TF = word_dict[key] / len(indexed_document[i])\n",
        "            IDF = math.log(len(indexed_document) / word_document[key])\n",
        "            weight.append(TF * IDF)\n",
        "            start.append(key)\n",
        "            end.append(i)\n",
        "            weight.append(TF*IDF)\n",
        "\n",
        "    return start, end, weight\n",
        "\n",
        "\n",
        "def build_graph(index_context: list, window_size):\n",
        "    windows = []\n",
        "    for line in tqdm(index_context):\n",
        "        if len(line) <= window_size:\n",
        "            windows.append(line)\n",
        "        else:\n",
        "            for i in range(len(line) - window_size + 1):\n",
        "                windows.append(line[i:i + window_size])\n",
        "    print('generating word relations')\n",
        "    single_word_fluency, tuple_word_fluency = word_window_num(windows)\n",
        "    window_num = len(windows)\n",
        "    start = []\n",
        "    end = []\n",
        "    weight = []\n",
        "    for sen in tuple_word_fluency.keys():\n",
        "        s,t=sen.split(',')\n",
        "        s,t=int(s),int(t)\n",
        "        score = math.log(tuple_word_fluency[str(s)+','+str(t)] / window_num / (\n",
        "                single_word_fluency[s] / window_num * single_word_fluency[t] / window_num))\n",
        "        if score < 0:\n",
        "            continue\n",
        "        start.append(s)\n",
        "        end.append(t)\n",
        "        weight.append(score)\n",
        "    start, end, weight = word_document(index_context, start, end, weight)\n",
        "    return start, end, weight\n",
        "\n",
        "dataset = dataet_name\n",
        "file_path = '/content/data/' + dataset + '.txt'\n",
        "with open(file_path) as f:\n",
        "    lines = f.readlines()\n",
        "titles = lines\n",
        "orig_titles = copy.deepcopy(titles)\n",
        "content_path = '/content/data/corpus/' + dataset + '.clean.txt'\n",
        "content = open(content_path).readlines()\n",
        "shuffle(titles)\n",
        "indexs = [orig_titles.index(each) for each in titles]\n",
        "content=[content[i] for i in indexs]\n",
        "dictonary, index_data = convert_data_to_index(content)\n",
        "train_index = []\n",
        "test_index = []\n",
        "label_dict = {}\n",
        "labels = []\n",
        "for i in range(len(indexs)):\n",
        "    line = titles[i].strip().split()\n",
        "    if line[1] == 'train':\n",
        "        train_index.append(i)\n",
        "    else:\n",
        "        test_index.append(i)\n",
        "    label = line[-1]\n",
        "    if label not in label_dict.keys():\n",
        "        label_dict[label] = len(label_dict)\n",
        "    labels.append(label_dict[label])\n",
        "shuffle(train_index)\n",
        "valid_index = train_index[int(len(train_index) * 0.9):]\n",
        "train_index = train_index[:int(len(train_index) * 0.9)]\n",
        "start, end, weight = build_graph(index_data, 20)\n",
        "matrix = coo_matrix((weight, (start, end)))\n",
        "\n",
        "pickle.dump(matrix, open('/content/data/' + dataset + '_matrix.pkl', 'wb'))\n",
        "pickle.dump(indexs, open('/content/data/' + dataset + '_indexs.pkl', 'wb'))\n",
        "pickle.dump(train_index, open('/content/data/' +dataset + '_train_index.pkl', 'wb'))\n",
        "pickle.dump(test_index, open('/content/data/' + dataset + '_test_index.pkl', 'wb'))\n",
        "pickle.dump(valid_index, open('/content/data/' + dataset + '_valid_index.pkl', 'wb'))\n",
        "pickle.dump(labels, open('/content/data/' + dataset + '_labels.pkl', 'wb'))\n",
        "pickle.dump(dictonary, open('/content/data/' + dataset + '_dict.pkl', 'wb'))\n",
        "pickle.dump(label_dict, open('/content/data/' +dataset + '_label_dict.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_q6Tzfw02uC"
      },
      "source": [
        "## finetune phobert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165,
          "referenced_widgets": [
            "1d8c7ecffc5c458e9123a66899e9aed2",
            "022e47c5e43c4819954f49093d599556",
            "158bbc75a6bd4fb48d5e29ef27d990e4",
            "a858436bd854434e8488155b79a3576a",
            "ee9aac617fbe4a29aafb6c4362d4ec9e",
            "49f13c1a6c2b454d9d2c7ce422b7110d",
            "3dd770986f654512a4f20ade16387566",
            "fabcb590fba141cdaafa16df254d2f1c",
            "03872aed21cb4421ab9d7da5e88573a2",
            "db136a66822f4268a7cba92f858c3f73",
            "cef2a528da814cc09199e246b84f82a1",
            "8c657115cd5148418615a8a9f93456d8",
            "c9d4e45303584985b61ff33518d8667b",
            "56bb980bf709416995d13c1b4b77a91f",
            "9d181b0fae01416db0c20ba3889c777b",
            "af141b1075d348b19738666a53cb8dc1",
            "fd6780524a814e909ad2594b0f175352",
            "b32a65b278ad42cd9577234dc0be4625",
            "024088a05aea49399a1fb11a22d6dedb",
            "c41be55381e244cba1731719a338b488",
            "bd2d06a8e0914b19ae457e06f4cccdfd",
            "095a8ae8fa8c462a9965de7ae7cfc834",
            "fe3bbbfb145242168fde71130e18e48d",
            "549a43ada3664787b99722bfe48669dd",
            "2642c168171f4df5a3f5d17b350887bf",
            "3ac15c4c7d514c9cab2a72ddfa415ff8",
            "092897ab1cde4a00b99f76608656663d",
            "66b1b219e46e4728b8d1e00d1e2b77bf",
            "33dbf1c750cb4a9fbd9f2f9662f230e5",
            "1182d0db0bd9414cb193555c9a1acc9a",
            "daa69d800fbd4cd68b5282bdf46d2703",
            "3f9387f78905474f82486f610af7e136",
            "8b56ac6acc63441298526886d31f47af"
          ]
        },
        "id": "UTxSykia035L",
        "outputId": "c9d64fe1-7be7-4063-a134-d1e514dbd521"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d8c7ecffc5c458e9123a66899e9aed2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/557 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c657115cd5148418615a8a9f93456d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/895k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe3bbbfb145242168fde71130e18e48d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "def collate_fn(batch):\n",
        "    src, label = [], []\n",
        "    for s, t in batch:\n",
        "        src.append(s)\n",
        "        label.append(t)\n",
        "    src = pad_sequence(src, batch_first=True, padding_value=0)\n",
        "    label = torch.tensor(label)\n",
        "    return src, label\n",
        "\n",
        "\n",
        "def train(i, model, optim, data_loader, device):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "    total=0\n",
        "    for src, trg in tqdm(data_loader):\n",
        "        optim.zero_grad()\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        predict = model(src)\n",
        "        loss = cross_entropy(predict, trg.long())\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        losses.append(loss.item())\n",
        "        correct += (torch.argmax(predict, -1) == trg).sum().item()\n",
        "        total+=predict.shape[0]\n",
        "    print(\"train epoch {} accuracy {} || loss {}\".format(i, correct / total, mean(losses)))\n",
        "\n",
        "\n",
        "def eval(i, model, best_loss, no_increase, data_loader, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for src, trg in tqdm(data_loader):\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        predict = model(src)\n",
        "        loss = cross_entropy(predict, trg.long())\n",
        "        losses.append(loss.item())\n",
        "        correct += (torch.argmax(predict, -1) == trg).sum().item()\n",
        "        total += predict.shape[0]\n",
        "    loss = mean(losses)\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        torch.save(model.state_dict(), 'best_pretrain_phobert.pkl')\n",
        "        no_increase = 0\n",
        "    else:\n",
        "        no_increase += 1\n",
        "    print(\"eval epoch {} accuracy {} || loss {}\".format(i, correct / total, mean(losses)))\n",
        "    return best_loss, no_increase\n",
        "\n",
        "\n",
        "def test( model, data_loader, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for src, trg in tqdm(data_loader):\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        predict = model(src)\n",
        "        loss = cross_entropy(predict, trg.long())\n",
        "        losses.append(loss.item())\n",
        "        correct += (torch.argmax(predict, -1) == trg).sum().item()\n",
        "        total += predict.shape[0]\n",
        "    loss = mean(losses)\n",
        "    print(\"test accuracy {} || loss {}\".format( correct / total, mean(losses)))\n",
        "\n",
        "\n",
        "dataset = dataet_name\n",
        "# args = get_args()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "data = DataSet(dataset, 'train')\n",
        "train_loader = DataLoader(data, collate_fn=collate_fn, batch_size=7,shuffle=True)\n",
        "valid_data = DataSet(dataset, 'valid')\n",
        "val_loader = DataLoader(valid_data, collate_fn=collate_fn, batch_size=7)\n",
        "test_data=DataSet(dataset,'test')\n",
        "test_loader=DataLoader(test_data,collate_fn=collate_fn,batch_size=7)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qOq0M5pxHuJO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUaTv-YzJ044"
      },
      "outputs": [],
      "source": [
        "model=finetunedphoBert(data.class_num)\n",
        "model = model.to(device)\n",
        "optim = AdamW(model.parameters(), lr=2e-5)\n",
        "best_loss = 1e10\n",
        "no_increasing = 0\n",
        "for i in range(12):\n",
        "    train(i, model, optim, train_loader, device)\n",
        "    with torch.no_grad():\n",
        "        no_increasing, best_loss = eval(i, model, best_loss, no_increasing, val_loader, device)\n",
        "        if no_increasing>3:\n",
        "            break\n",
        "model.load_state_dict(torch.load('best_pretrain_phobert.pkl'))\n",
        "with torch.no_grad():\n",
        "    test(model,test_loader,device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOb8_9is3XLw"
      },
      "source": [
        "# trainning process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OZUmhzZQ3S2e"
      },
      "outputs": [],
      "source": [
        "setup_seed(9)\n",
        "def train(i, dataset, model: phoBertGCN, optim, features, graph, device):\n",
        "    print(' epoch ',i+1)\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    increase=0\n",
        "    for src, label, attention,mask, index in tqdm(dataset):\n",
        "        mask = mask.to(device)\n",
        "        src = src.to(device)\n",
        "        attention=attention.to(device)\n",
        "        label = label.to(device)\n",
        "        predict = model(src, features,attention, graph, index)\n",
        "        predict = predict[mask]\n",
        "        label = label[mask]\n",
        "        if predict.shape[0] == 0:\n",
        "            continue\n",
        "        # print(predict,label)\n",
        "        loss = nll_loss(torch.log(predict), label)\n",
        "        loss.backward(retain_graph=True)\n",
        "        increase+=1\n",
        "        if increase%2==0:\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            increase=0\n",
        "        total += predict.shape[0]\n",
        "        correct += (torch.argmax(predict, -1) == label).sum().item()\n",
        "        losses.append(loss.item())\n",
        "        a = torch.Tensor.cpu(torch.argmax(predict, -1))\n",
        "        b = torch.Tensor.cpu(label)\n",
        "    print(\"training set : (loss {} || accuracy {}  || f1_score {})\".format(mean(losses), correct / total,f1_score(a,b,average= f1_type)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yLCgJvzlMvWS"
      },
      "outputs": [],
      "source": [
        "def update_features(features, dataset, model, device):\n",
        "    print('update featre')\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for src, label,attention, mask, idx in tqdm(dataset):\n",
        "            src = src.to(device)\n",
        "            attention=attention.to(device)\n",
        "            current_features = model.phoBertModel.model(src,attention_mask=attention).last_hidden_state[:, 0, :]\n",
        "            features[idx] = current_features.detach()\n",
        "    return features\n",
        "def eval(i, dataset, model: phoBertGCN, features, graph, usage, device, best_loss=None, best_accuracy=None,\n",
        "         no_increasing=None):\n",
        "    model.eval()\n",
        "    if usage == 'valid':\n",
        "        mask = graph.ndata['valid_mask']\n",
        "    else:\n",
        "        mask = graph.ndata['test_mask']\n",
        "    mask = (mask == 1)\n",
        "    if usage == 'test':\n",
        "        model.load_state_dict(torch.load('best_phoBert_GCN_model.pkl'))\n",
        "        features = update_features(features, dataset, model, device)\n",
        "    predict = model.phoBertModel.linear(features)\n",
        "    graph_predict = model.gcn(graph, features)\n",
        "    predict = softmax(predict[mask], -1) * (1 - model.lam) + softmax(graph_predict[mask], -1) * model.lam\n",
        "    label = graph.ndata['label']\n",
        "    loss = nll_loss(torch.log(predict), label[mask])\n",
        "    correct = (torch.argmax(predict, -1) == label[mask]).sum().item()\n",
        "    # print(label[mask])\n",
        "    a = torch.Tensor.cpu(torch.argmax(predict, -1))\n",
        "    b = torch.Tensor.cpu(label[mask])\n",
        "    # print(torch.argmax(predict, -1)\n",
        "    total = sum(mask).item()\n",
        "    print(\"{} loss {} || accuracy {} || f1_score {}\".format(usage, loss.item(), correct / total,f1_score(a,b,average= 'macro')))\n",
        "    if usage == 'valid':\n",
        "        if best_loss > loss.item():\n",
        "            best_loss = loss.item()\n",
        "            no_increasing = 0\n",
        "            torch.save(model.state_dict(), 'best_phoBert_GCN_model.pkl')\n",
        "            print(\"saving to file best_phoBert_GCN_model.pkl\")\n",
        "\n",
        "        else:\n",
        "            no_increasing += 1\n",
        "        if best_accuracy < correct / total:\n",
        "            best_accuracy = correct / total\n",
        "            torch.save(model.state_dict(), 'best_accuracy.pkl')\n",
        "            print(\"saving to file best_accuracy.pkl\")\n",
        "        return best_loss, best_accuracy, no_increasing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9VfWhAsv3bvA",
        "outputId": "3c5c7e59-1a09-4b5f-85aa-e8a9bd609f53"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dataset = Data_set(dataet_name)\n",
        "graph = dataset.graph.to(device)\n",
        "data_loader = DataLoader(dataset,  batch_size=7, shuffle=True)\n",
        "features = torch.zeros(graph.num_nodes(), 768, requires_grad=False).to(device)\n",
        "model = phoBertGCN('./best_pretrain_phobert.pkl', dataset.label_num)\n",
        "model = model.to(device)\n",
        "optim = torch.optim.Adam([{'params': model.gcn.parameters(), 'lr': 1e-3}, {'params': model.phoBertModel.parameters(), 'lr': 1e-5}])\n",
        "scheduler=lr_scheduler.MultiStepLR(optim,milestones=[30],gamma=0.1)\n",
        "features = update_features(features, data_loader, model, device)\n",
        "best_loss = 1e10\n",
        "no_increasing = 0\n",
        "best_accuracy = 0\n",
        "for i in range(6):\n",
        "    torch.cuda.empty_cache()\n",
        "    train(i, data_loader, model, optim, features, graph, device)\n",
        "    scheduler.step()\n",
        "    torch.cuda.empty_cache()\n",
        "    with torch.no_grad():\n",
        "        features = update_features(features, data_loader, model, device)\n",
        "        best_loss, best_accuracy, no_increasing = eval(i, data_loader, model, features, graph, 'valid', device,\n",
        "                                                        best_loss,\n",
        "                                                        best_accuracy,\n",
        "                                                        no_increasing)\n",
        "    if no_increasing >= 10:\n",
        "        break  # for i in range(20):\n",
        "with torch.no_grad():\n",
        "    eval(0, data_loader, model, features, graph, 'test', device, best_loss, best_accuracy, no_increasing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iawavg970ysc",
        "outputId": "f5acbf58-818b-4ea9-e707-fe737b66153f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "update featre\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 892/892 [01:27<00:00, 10.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test loss 0.6573890447616577 || accuracy 0.8268398268398268 || f1_score 0.8220649919418118\n"
          ]
        }
      ],
      "source": [
        "# model = phoBertGCN('/content/best_phoBert_GCN_model.pkl', dataset.label_num)\n",
        "\n",
        "with torch.no_grad():\n",
        "    eval(0, data_loader, model, features, graph, 'test', device, best_loss, best_accuracy, no_increasing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUFbxjukLxkf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-Qm_DII13AYh",
        "yQtcxg8Tz4-u",
        "AOpewssfyyRi",
        "Rjon6T-OtmPb",
        "47A_nGouvJVi",
        "DuY6Va1TmOTd",
        "cAmJ-_ndwFag",
        "rlbcFsY4y7Vb",
        "IepFinMmzJXb",
        "P_q6Tzfw02uC"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "022e47c5e43c4819954f49093d599556": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49f13c1a6c2b454d9d2c7ce422b7110d",
            "placeholder": "​",
            "style": "IPY_MODEL_3dd770986f654512a4f20ade16387566",
            "value": "Downloading: 100%"
          }
        },
        "024088a05aea49399a1fb11a22d6dedb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03872aed21cb4421ab9d7da5e88573a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "092897ab1cde4a00b99f76608656663d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "095a8ae8fa8c462a9965de7ae7cfc834": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1182d0db0bd9414cb193555c9a1acc9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "158bbc75a6bd4fb48d5e29ef27d990e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fabcb590fba141cdaafa16df254d2f1c",
            "max": 557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03872aed21cb4421ab9d7da5e88573a2",
            "value": 557
          }
        },
        "1d8c7ecffc5c458e9123a66899e9aed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_022e47c5e43c4819954f49093d599556",
              "IPY_MODEL_158bbc75a6bd4fb48d5e29ef27d990e4",
              "IPY_MODEL_a858436bd854434e8488155b79a3576a"
            ],
            "layout": "IPY_MODEL_ee9aac617fbe4a29aafb6c4362d4ec9e"
          }
        },
        "2642c168171f4df5a3f5d17b350887bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1182d0db0bd9414cb193555c9a1acc9a",
            "max": 1135173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_daa69d800fbd4cd68b5282bdf46d2703",
            "value": 1135173
          }
        },
        "33dbf1c750cb4a9fbd9f2f9662f230e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ac15c4c7d514c9cab2a72ddfa415ff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f9387f78905474f82486f610af7e136",
            "placeholder": "​",
            "style": "IPY_MODEL_8b56ac6acc63441298526886d31f47af",
            "value": " 1.14M/1.14M [00:01&lt;00:00, 909kB/s]"
          }
        },
        "3dd770986f654512a4f20ade16387566": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f9387f78905474f82486f610af7e136": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49f13c1a6c2b454d9d2c7ce422b7110d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "549a43ada3664787b99722bfe48669dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66b1b219e46e4728b8d1e00d1e2b77bf",
            "placeholder": "​",
            "style": "IPY_MODEL_33dbf1c750cb4a9fbd9f2f9662f230e5",
            "value": "Downloading: 100%"
          }
        },
        "56bb980bf709416995d13c1b4b77a91f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_024088a05aea49399a1fb11a22d6dedb",
            "max": 895321,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c41be55381e244cba1731719a338b488",
            "value": 895321
          }
        },
        "66b1b219e46e4728b8d1e00d1e2b77bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b56ac6acc63441298526886d31f47af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c657115cd5148418615a8a9f93456d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9d4e45303584985b61ff33518d8667b",
              "IPY_MODEL_56bb980bf709416995d13c1b4b77a91f",
              "IPY_MODEL_9d181b0fae01416db0c20ba3889c777b"
            ],
            "layout": "IPY_MODEL_af141b1075d348b19738666a53cb8dc1"
          }
        },
        "9d181b0fae01416db0c20ba3889c777b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd2d06a8e0914b19ae457e06f4cccdfd",
            "placeholder": "​",
            "style": "IPY_MODEL_095a8ae8fa8c462a9965de7ae7cfc834",
            "value": " 895k/895k [00:01&lt;00:00, 957kB/s]"
          }
        },
        "a858436bd854434e8488155b79a3576a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db136a66822f4268a7cba92f858c3f73",
            "placeholder": "​",
            "style": "IPY_MODEL_cef2a528da814cc09199e246b84f82a1",
            "value": " 557/557 [00:00&lt;00:00, 37.6kB/s]"
          }
        },
        "af141b1075d348b19738666a53cb8dc1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b32a65b278ad42cd9577234dc0be4625": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd2d06a8e0914b19ae457e06f4cccdfd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c41be55381e244cba1731719a338b488": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9d4e45303584985b61ff33518d8667b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd6780524a814e909ad2594b0f175352",
            "placeholder": "​",
            "style": "IPY_MODEL_b32a65b278ad42cd9577234dc0be4625",
            "value": "Downloading: 100%"
          }
        },
        "cef2a528da814cc09199e246b84f82a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daa69d800fbd4cd68b5282bdf46d2703": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db136a66822f4268a7cba92f858c3f73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee9aac617fbe4a29aafb6c4362d4ec9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fabcb590fba141cdaafa16df254d2f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd6780524a814e909ad2594b0f175352": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe3bbbfb145242168fde71130e18e48d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_549a43ada3664787b99722bfe48669dd",
              "IPY_MODEL_2642c168171f4df5a3f5d17b350887bf",
              "IPY_MODEL_3ac15c4c7d514c9cab2a72ddfa415ff8"
            ],
            "layout": "IPY_MODEL_092897ab1cde4a00b99f76608656663d"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
